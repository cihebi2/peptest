# PepLand 项目全面升级规划与路线图（“PepLand 2.0”）

## 目标与愿景

- 构建覆盖标准与非标准氨基酸的统一、可扩展、可解释的肽类分子基础模型（Foundation Model），在表征学习、下游预测与条件生成三方面全面超越现有 PepLand。
- 在保持现有异构图多视角优势（原子/片段/连接）基础上，引入几何信息（3D）、跨模态/跨层级对齐、规模化自监督任务与高效工程化训练，成为肽工程研发的一体化平台。

## 现状评估（基于 docs/项目技术调研报告.md 与代码）

- 数据与任务：两阶段预训练（canonical → non-canonical），多下游数据集（PPI、CPP、溶解度等）；评测以分类/回归常用指标为主。
- 表示与建模：DGL 异构图 + 多视角消息传递（MVMP）+ 多头注意力；片段词表基于手工/预制词典（258/410）。
- 训练与工程：DDP、MLflow 追踪、YAML 配置，批量化较好，尚未覆盖深度模型并行/内存优化方案。
- 推理与应用：提供特征抽取器与属性预测器（MLP 头），未包含生成式能力与更丰富推理加速路径。

主要短板与机会：

- 缺少 3D 几何与等变建模，难以刻画构象/空间相互作用；
- 预训练任务以掩码为主，未覆盖对比/生成/时序（聚合顺序）等多种自监督信号；
- 词表与切分以规则为主，泛化到复杂非标准/环肽/修饰肽仍有上限；
- 无条件生成与属性约束生成链路，难以形成设计闭环；
- 工程侧未引入更先进的分布式/内存优化、配置/数据版本化、系统化测试与数据泄漏防控。

## 总体技术路线

- 多视角 × 几何等变 × 多任务预训练：在原子/片段/连接的异构图上，融合 3D 坐标/距离/角度/扭转特征，引入 SE(3)-等变模块与图 Transformer，联合掩码、对比、重建、生成式任务预训练。
- 数据驱动的分子子结构分词：结合 SELFIES/SMILES 与图结构，用 SentencePiece/BPE 学习“片段词表”，增强对非标准氨基酸/修饰肽的覆盖。
- 条件生成与优化：图扩散/自回归生成 + 性能预测器（或奖励模型）形成优化闭环，可控生成满足溶解度、CPP、结合能等约束。
- 规模化训练与MLOps：DeepSpeed/Accelerate、混合精度、梯度检查点、DVC + MLflow + Hydra，完善测试与数据治理。

## 关键改进方向

### 1. 数据与知识增强

- 构象与几何：
  - 对预训练与下游样本使用 RDKit ETKDG 生成多构象，存储 3D 坐标、键角、二面角、片段质心距离矩阵；
  - 对短肽可引入快速半经验/分子力场能量作为附加教师信号（离线）。
- 多源数据扩充：
  - 聚合 PepAtlas、UniParc、专利/文献中肽库、商业/学术合成库（去重、标准化、合规过滤）；
  - 引入 HELM/Monomer 库，统一标准/非标准单体表示与别名映射。
- 数据治理与拆分：
  - Scaffold/时间/库外（OOS）多策略拆分；
  - 明确训练/验证/测试数据去重与泄漏扫描（SMILES 归一、图同构、子结构相似）。

### 2. Tokenizer 与表征层

- 统一多表示：SMILES/SELFIES/图三通道输入；
- 数据驱动词表：
  - GraphPiece/SentencePiece-on-Graphs：在图上采样子图作为“子词”，学习 5k–50k 规模词表；
  - 支持 HELM 单体粒度（保护基/桥连/环化等修饰显式化）。
- 多粒度对齐：单体/片段/原子级别交互注意力，对齐不同层次语义。

### 3. 模型架构（PepLand 2.0 主干）

- 几何等变与图 Transformer 融合：
  - 引入 EGNN/SE(3)-Transformer/GeoTransformer 模块，编码 3D 坐标；
  - 在现有 MVMP 基础上增加几何注意力（距离/角度/扭转/RBF 编码）。
- 关系与全局上下文：
  - 加入全局令牌（[G]) 做跨视角汇聚（原子/片段/单体/序列）；
  - 片段-片段、单体-单体的动态路由注意力，显式建模远程依赖。
- 兼容性与可插拔：
  - 通过注册表/工厂模式接入 backbone：HGT、Graphormer、EGNN、Hybrid 可选；
  - Head 模块标准化：分类/回归/打分/生成头统一接口。

### 4. 预训练任务体系

- 掩码（扩展）：原子/键/片段/单体/序列 token 同/异构联合掩码；
- 对比：
  - 视角对比（原子↔片段↔单体↔序列）及图-序列/图-文本（描述性标签）对比；
  - 构象对比（不同 conformer 的一致性约束）。
- 重建与去噪：
  - 图自编码/去噪（节点/边/3D 坐标噪声恢复）；
  - 片段顺序预测（合成/聚合顺序建模）。
- 生成：
  - 图扩散（EDM/DiGress）或自回归（GraphGPT）损失联合训练；
  - 条件提示：性质条件、结构 motif、长度/单体约束、多目标软约束（可用 soft-constraint penalty）。
- 知识蒸馏：
  - 从 QM/MD 离线计算得到的电荷/能量/二面角分布对表征进行蒸馏；
  - 从高容量教师（例如更大规模图 Transformer）蒸馏到部署模型。

### 5. 下游任务与评测体系

- 任务扩展：
  - CPP、PPI、溶解度之外：蛋白酶稳定性、血浆半衰期、细胞毒性/溶血、免疫原性、二级结构偏好、膜通透/运输体结合、常见靶点亲和力（整合公开/内部数据）。
- 数据拆分与鲁棒性：
- Scaffold/库外/OOS 基准；交叉物种或不同测序平台域外泛化；
  - 校验数据泄漏、重复构象/异构体影响。
- 统一指标与 Leaderboard：
  - 分类：AUC/PR-AUC/F1/Recall@K；回归：RMSE/MAE/Pearson/Spearman；
  - 生成：有效率、唯一率、新颖率、约束满足率、目标提升幅度。

### 6. 生成式设计闭环

- 生成器：
  - 图扩散（带几何）或自回归图生成（节点/边/坐标）；
  - 约束：长度、单体集合、特定片段/交联、环化策略。
- 优化与微调：
  - 以属性预测器/多目标奖励为指导，做 RL（PPO/REINFORCE）或离线策略优化（CQL/BCQ）；
  - DPO/ORPO 等对齐范式，使生成分布更贴合专家偏好或实验先验。

### 7. 训练与工程化

- 训练效率：
  - AMP/mixed-precision、梯度检查点、Flash-Attn、Fused Adam、
    DeepSpeed ZeRO-2/3；按节点/设备自动并行（pipeline/tensor 并行）；
  - 大批次 + LARS/AdamW，学习率热身 + 余弦退火/OneCycle。
- 配置与复现实验：
  - Hydra 配置（多配置组合/覆盖）；DVC 数据版本；MLflow 继续追踪；
  - 固化随机种子、容器化（Dockerfile + environment.yaml）。
- 质量保障：
  - 数据单测（采样/掩码/切分）、模型单测（前向/形状）、指标单测；
  - 预提交钩子（黑/ruff/isort/mypy）、CI（lint + 单测 + 小样本训练）。

### 8. 推理与部署

- 模型压缩：蒸馏小模型、LoRA 适配、8/4-bit 量化（AWQ/QLoRA 风格）；
- 导出与加速：TorchScript/ONNX/TensorRT、DGLGraph → 专用 runtime；
- 服务化：统一 `inference.py` 接口，封装批量/流式生成与属性评分。

### 9. 可解释性与安全

- 解释：子结构归因（IG/Grad-CAM-on-Graphs）、对齐可视化（原子↔片段↔单体）、构象灵敏度分析；
- 安全与合规：数据许可、序列/结构敏感信息脱敏，生成样本过滤（PAINS/ REOS/毒性规则）。

## 里程碑与交付物

- M0（第0–2周）：工程与数据基线
  - 交付：数据治理与拆分脚本、泄漏扫描、CI/测试框架、Hydra + DVC 接入；
- M1（第3–6周）：Tokenizer 与数据扩展
  - 交付：GraphPiece 词表、HELM 单体统一映射、SELFIES/SMILES/图三通道管线；
- M2（第7–14周）：几何 + 多任务预训练（PepLand 2.0 主干）
  - 交付：几何等变骨干、联合自监督损失、首版 50M 样本预训练 checkpoint；
- M3（第15–18周）：下游统一评测与SOTA对比
  - 交付：统一 benchmark 与排行榜、论文级 ablation；
- M4（第19–22周）：生成式设计与闭环优化
  - 交付：条件图扩散生成器、属性约束优化、有效案例库；
- M5（第23–24周）：推理部署与文档化
  - 交付：压缩/量化/导出、推理服务、用户指南与API。

## 资源预估（参考）

- 计算：A100 80G × 8–32（预训练阶段）；SSD 20–50 TB；
- 人员：
  - 研究/算法 2–3、平台/工程 2、数据/评测 1–2、应用/化学 1–2；
- 时间：~6 个月达成“PepLand 2.0 + 生成闭环”首版。

## 风险与缓解

- 3D 构象质量与覆盖：采用多构象集合训练 + 构象对比一致性；
- 非标准单体复杂度：HELM/Monomer 库维护与自动规范化；
- 数据泄漏/评测偏差：多策略拆分 + 泄漏扫描 + 外部基准复核；
- 训练成本：分阶段预训练、蒸馏/剪枝、增量继续训练与检查点复用。

## 与现有代码的改造映射（建议清单）

- 数据与Tokenizer：
  - 新增 `tokenizer/graphpiece_builder.py`（学习子图词表）、`tokenizer/helm_monomer.py`（单体规范化）、`data/conformers.py`（构象生成）。
- 模型：
  - 新增 `model/geom_transformer.py`（EGNN/SE(3)模块）、`model/backbones/__init__.py`（注册表）、`model/heads/*`（分类/回归/生成头）；
  - 拆分 `model/model.py` 为“骨干 + 视角路由 + 读出”的可插拔结构。
- 训练：
  - 扩展 `trainer.py` → 多任务损失管理、DeepSpeed/Accelerate 支持；
  - 新增 `configs/hydra/*` 与多场景 yaml（预训练/下游/生成/蒸馏）。
- 推理：
  - 扩展 `inference.py` 支持批量特征、生成、约束；导出/量化脚本 `tools/export_onnx.py`、`tools/quantize.py`。
- MLOps：
  - 新增 `tests/*`（数据/模型/指标单测）、`scripts/data_checks.py`（泄漏扫描）、`pre-commit-config.yaml`。

## 优先级建议（先做什么）

1) 立刻落地数据治理 + 多策略拆分 + CI/测试（M0）；
2) 词表与Tokenizer升级（GraphPiece + HELM）（M1）；
3) 引入几何等变 + 多任务预训练主干（M2）；
4) 统一评测与可解释性（M3）；
5) 条件生成闭环（M4）；
6) 推理压缩与服务化（M5）。

---

如需，我可以基于上述路线先行提交 M0/M1 的具体代码改造 PR（数据治理脚本、GraphPiece 词表管线与 HELM 规范化），并对现有配置/训练脚本完成最小侵入式重构。