# è¶…è¶Š PepLand çš„å…¨é¢æ”¹è¿›ç­–ç•¥

## ç›®å½•
- [1. PepLand ç°çŠ¶åˆ†æ](#1-pepland-ç°çŠ¶åˆ†æ)
- [2. æ ¸å¿ƒæ”¹è¿›æ–¹å‘](#2-æ ¸å¿ƒæ”¹è¿›æ–¹å‘)
- [3. æ¨¡å‹æ¶æ„å‡çº§](#3-æ¨¡å‹æ¶æ„å‡çº§)
- [4. é¢„è®­ç»ƒç­–ç•¥ä¼˜åŒ–](#4-é¢„è®­ç»ƒç­–ç•¥ä¼˜åŒ–)
- [5. ç‰¹å¾å·¥ç¨‹å¢å¼º](#5-ç‰¹å¾å·¥ç¨‹å¢å¼º)
- [6. è®­ç»ƒæŠ€å·§ä¼˜åŒ–](#6-è®­ç»ƒæŠ€å·§ä¼˜åŒ–)
- [7. ä¸‹æ¸¸ä»»åŠ¡é€‚é…](#7-ä¸‹æ¸¸ä»»åŠ¡é€‚é…)
- [8. å®æ–½è·¯çº¿å›¾](#8-å®æ–½è·¯çº¿å›¾)
- [9. é¢„æœŸæ€§èƒ½æå‡](#9-é¢„æœŸæ€§èƒ½æå‡)

---

## 1. PepLand ç°çŠ¶åˆ†æ

### 1.1 ä¼˜åŠ¿åˆ†æ

âœ… **åˆ›æ–°ç‚¹**ï¼š
1. å¤šè§†å›¾å¼‚æ„å›¾è¡¨ç¤ºï¼ˆåŸå­+ç‰‡æ®µï¼‰
2. AdaFragåˆ†ç‰‡ç®—æ³•ï¼ˆé’ˆå¯¹è‚½æ®µä¼˜åŒ–ï¼‰
3. ä¸¤é˜¶æ®µé¢„è®­ç»ƒï¼ˆè§„èŒƒâ†’éè§„èŒƒæ°¨åŸºé…¸ï¼‰
4. å®Œæ•´çš„å®ç°æ¡†æ¶

âœ… **æŠ€æœ¯äº®ç‚¹**ï¼š
- å¼‚æ„å›¾ç¥ç»ç½‘ç»œï¼ˆHGTï¼‰
- å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- æ¶ˆæ¯ä¼ é€’èŒƒå¼

### 1.2 å±€é™æ€§åˆ†æ

âŒ **æ¨¡å‹æ¶æ„å±€é™**ï¼š
```python
# 1. HGTæ¶æ„ç›¸å¯¹åŸºç¡€ï¼ˆ2020å¹´æå‡ºï¼‰
- æœªä½¿ç”¨æœ€æ–°çš„Graph Transformer
- æ³¨æ„åŠ›æœºåˆ¶æ•ˆç‡è¾ƒä½ï¼ˆO(nÂ²)å¤æ‚åº¦ï¼‰
- ç¼ºå°‘ä½ç½®ç¼–ç å’Œç»“æ„ç¼–ç 

# 2. å›¾è¯»å‡ºç®€å•
- ä»…ä½¿ç”¨GRU + å¹³å‡æ± åŒ–
- æœªè€ƒè™‘å±‚æ¬¡åŒ–è¡¨ç¤º
- ç¼ºå°‘å…¨å±€-å±€éƒ¨äº¤äº’

# 3. ç‰¹å¾è¡¨ç¤ºä¸è¶³
- ä»…ä½¿ç”¨2Dæ‹“æ‰‘ç»“æ„
- ç¼ºå°‘3Dæ„è±¡ä¿¡æ¯
- æœªåˆ©ç”¨ç‰©ç†åŒ–å­¦æ€§è´¨
```

âŒ **é¢„è®­ç»ƒç­–ç•¥å±€é™**ï¼š
```python
# 1. è‡ªç›‘ç£ä»»åŠ¡å•ä¸€
- ä»…ä½¿ç”¨Masked Prediction
- ç¼ºå°‘å¯¹æ¯”å­¦ä¹ 
- æœªåˆ©ç”¨åºåˆ—ä¿¡æ¯

# 2. æ•°æ®å¢å¼ºç®€å•
- ä»…æœ‰éšæœºé®è”½
- æœªä½¿ç”¨åˆ†å­å±‚é¢å¢å¼º
- ç¼ºå°‘éš¾æ ·æœ¬æŒ–æ˜

# 3. è®­ç»ƒæ•ˆç‡é—®é¢˜
- æœªä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
- ç¼ºå°‘æ¢¯åº¦ç´¯ç§¯
- æ•°æ®åŠ è½½å¯èƒ½æˆä¸ºç“¶é¢ˆ
```

âŒ **ä¸‹æ¸¸é€‚é…ä¸è¶³**ï¼š
```python
# 1. å¾®è°ƒç­–ç•¥ç®€å•
- ä»…æ·»åŠ MLPå¤´
- æœªä½¿ç”¨Adapter/LoRAç­‰é«˜æ•ˆå¾®è°ƒ
- ç¼ºå°‘ä»»åŠ¡ç‰¹å®šçš„å½’çº³åç½®

# 2. ç¼ºå°‘å¤šä»»åŠ¡å­¦ä¹ 
- æ¯ä¸ªä»»åŠ¡ç‹¬ç«‹è®­ç»ƒ
- æœªå…±äº«çŸ¥è¯†

# 3. æ³›åŒ–èƒ½åŠ›å¾…æå‡
- å¯¹åˆ†å¸ƒå¤–æ•°æ®æ•æ„Ÿ
- å°æ ·æœ¬å­¦ä¹ èƒ½åŠ›å¼±
```

### 1.3 æ€§èƒ½åŸºçº¿ä¼°ç®—

æ ¹æ®è®ºæ–‡å’Œä»£ç åˆ†æï¼ŒPepLandåœ¨ä¸»è¦ä»»åŠ¡ä¸Šçš„é¢„æœŸæ€§èƒ½ï¼š

| ä»»åŠ¡ | æŒ‡æ ‡ | é¢„æœŸåŸºçº¿ |
|------|------|----------|
| Binding Affinity | Pearson R | 0.65-0.70 |
| Cell Penetration | AUC | 0.75-0.80 |
| Solubility | RMSE | 1.2-1.5 |
| Synthesizability | Accuracy | 0.70-0.75 |

**æ”¹è¿›ç›®æ ‡ï¼šåœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šæå‡ 5-15%**

---

## 2. æ ¸å¿ƒæ”¹è¿›æ–¹å‘

### 2.1 æ”¹è¿›ç­–ç•¥çŸ©é˜µ

| æ”¹è¿›ç»´åº¦ | é¢„æœŸæå‡ | å®æ–½éš¾åº¦ | ç®—åŠ›éœ€æ±‚ | ä¼˜å…ˆçº§ |
|----------|----------|----------|----------|--------|
| **æ¨¡å‹æ¶æ„** | 10-15% | â­â­â­ | 1.5x | ğŸ”¥ğŸ”¥ğŸ”¥ |
| **é¢„è®­ç»ƒä»»åŠ¡** | 5-10% | â­â­ | 1.2x | ğŸ”¥ğŸ”¥ğŸ”¥ |
| **å¯¹æ¯”å­¦ä¹ ** | 8-12% | â­â­â­ | 1.3x | ğŸ”¥ğŸ”¥ğŸ”¥ |
| **3Dæ„è±¡** | 5-8% | â­â­â­â­ | 2.0x | ğŸ”¥ğŸ”¥ |
| **é›†æˆå­¦ä¹ ** | 3-5% | â­ | 2.0x | ğŸ”¥ |
| **æ•°æ®å¢å¼º** | 3-7% | â­â­ | 1.1x | ğŸ”¥ğŸ”¥ |
| **å¾®è°ƒç­–ç•¥** | 5-8% | â­â­ | 1.0x | ğŸ”¥ğŸ”¥ğŸ”¥ |

### 2.2 èµ„æºåˆ†é…ç­–ç•¥

åŸºäº **4Ã—RTX 4090** çš„ç®—åŠ›é¢„ç®—ï¼š

```python
æ€»ç®—åŠ›: 4 Ã— 165 TFLOPS (FP16) = 660 TFLOPS

åˆ†é…æ–¹æ¡ˆ:
â”œâ”€â”€ ä¸»æ¨¡å‹è®­ç»ƒ: 2å¡ (50% æ—¶é—´)
â”œâ”€â”€ å¯¹æ¯”å­¦ä¹ è®­ç»ƒ: 2å¡ (30% æ—¶é—´)
â”œâ”€â”€ å¤šä»»åŠ¡å¾®è°ƒ: 1å¡ (15% æ—¶é—´)
â””â”€â”€ é›†æˆæ¨¡å‹è®­ç»ƒ: 1å¡ (5% æ—¶é—´)

æ€»è®­ç»ƒæ—¶é—´é¢„ç®—: 40-60 å°æ—¶
```

---

## 3. æ¨¡å‹æ¶æ„å‡çº§

### 3.1 æ ¸å¿ƒæ¶æ„ï¼šGraph Transformer with Performer

**ç­–ç•¥ï¼šä½¿ç”¨ Graphormer + Performer æ›¿ä»£ HGT**

#### 3.1.1 Graphormer æ¶æ„

```python
class GraphormerLayer(nn.Module):
    """
    åŸºäº Microsoft Graphormer (NIPS 2021)
    ä¼˜åŠ¿ï¼š
    1. ä¸­å¿ƒæ€§ç¼–ç ï¼ˆCentrality Encodingï¼‰
    2. ç©ºé—´ç¼–ç ï¼ˆSpatial Encodingï¼‰
    3. è¾¹ç‰¹å¾èå…¥æ³¨æ„åŠ›
    """
    def __init__(self, hidden_dim=512, num_heads=8, dropout=0.1):
        super().__init__()

        # 1. å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼ˆå¸¦è¾¹ç‰¹å¾ï¼‰
        self.attention = MultiHeadAttentionWithEdge(
            hidden_dim, num_heads, dropout
        )

        # 2. ä¸­å¿ƒæ€§ç¼–ç 
        self.centrality_encoding = nn.Embedding(100, hidden_dim)

        # 3. ç©ºé—´ç¼–ç ï¼ˆæœ€çŸ­è·¯å¾„è·ç¦»ï¼‰
        self.spatial_encoding = nn.Embedding(512, num_heads)

        # 4. è¾¹ç¼–ç 
        self.edge_encoding = nn.Linear(edge_dim, num_heads)

        # 5. FFN
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 4, hidden_dim),
            nn.Dropout(dropout)
        )

        # 6. Layer Norm
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.norm2 = nn.LayerNorm(hidden_dim)

    def forward(self, x, edge_attr, spatial_pos, in_degree, out_degree):
        # æ·»åŠ ä¸­å¿ƒæ€§ç¼–ç 
        x = x + self.centrality_encoding(in_degree) + \
                self.centrality_encoding(out_degree)

        # è®¡ç®—ç©ºé—´å’Œè¾¹çš„åç½®
        spatial_bias = self.spatial_encoding(spatial_pos)  # [B, N, N, H]
        edge_bias = self.edge_encoding(edge_attr)          # [B, N, N, H]
        attn_bias = spatial_bias + edge_bias

        # è‡ªæ³¨æ„åŠ›
        x_attn = self.attention(x, x, x, attn_bias)
        x = self.norm1(x + x_attn)

        # FFN
        x_ffn = self.ffn(x)
        x = self.norm2(x + x_ffn)

        return x


class MultiHeadAttentionWithEdge(nn.Module):
    """å¸¦è¾¹ç‰¹å¾çš„å¤šå¤´æ³¨æ„åŠ›"""
    def __init__(self, hidden_dim, num_heads, dropout=0.1):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads

        self.q_proj = nn.Linear(hidden_dim, hidden_dim)
        self.k_proj = nn.Linear(hidden_dim, hidden_dim)
        self.v_proj = nn.Linear(hidden_dim, hidden_dim)
        self.o_proj = nn.Linear(hidden_dim, hidden_dim)

        self.dropout = nn.Dropout(dropout)

    def forward(self, q, k, v, attn_bias=None):
        B, N, D = q.shape
        H = self.num_heads

        q = self.q_proj(q).view(B, N, H, -1).transpose(1, 2)  # [B, H, N, D/H]
        k = self.k_proj(k).view(B, N, H, -1).transpose(1, 2)
        v = self.v_proj(v).view(B, N, H, -1).transpose(1, 2)

        # æ³¨æ„åŠ›åˆ†æ•°
        attn = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)

        # æ·»åŠ è¾¹åç½®
        if attn_bias is not None:
            attn = attn + attn_bias.permute(0, 3, 1, 2)  # [B, H, N, N]

        attn = F.softmax(attn, dim=-1)
        attn = self.dropout(attn)

        # èšåˆ
        out = torch.matmul(attn, v)  # [B, H, N, D/H]
        out = out.transpose(1, 2).contiguous().view(B, N, D)
        out = self.o_proj(out)

        return out
```

#### 3.1.2 Performer ä¼˜åŒ–ï¼ˆé™ä½å¤æ‚åº¦ï¼‰

```python
class PerformerAttention(nn.Module):
    """
    ä½¿ç”¨ Performer (ICLR 2021) é™ä½æ³¨æ„åŠ›å¤æ‚åº¦
    å¤æ‚åº¦: O(nÂ²) â†’ O(n)
    """
    def __init__(self, hidden_dim, num_heads, nb_features=256):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_dim // num_heads
        self.nb_features = nb_features

        self.q_proj = nn.Linear(hidden_dim, hidden_dim)
        self.k_proj = nn.Linear(hidden_dim, hidden_dim)
        self.v_proj = nn.Linear(hidden_dim, hidden_dim)
        self.o_proj = nn.Linear(hidden_dim, hidden_dim)

        # éšæœºç‰¹å¾æŠ•å½±
        self.create_projection = lambda: torch.randn(
            self.nb_features, self.head_dim
        )

    def kernel_feature_creator(self, data, projection_matrix):
        """FAVOR+ æ ¸ç‰¹å¾æ˜ å°„"""
        data_normalizer = 1.0 / torch.sqrt(
            torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        )
        data = data_normalizer * data

        ratio = 1.0 / torch.sqrt(
            torch.tensor(self.nb_features, dtype=torch.float32)
        )

        data_dash = torch.einsum('...nd,md->...nm', data, projection_matrix)

        diag_data = torch.sum(data ** 2, dim=-1, keepdim=True) / 2.0
        diag_data = diag_data.expand_as(data_dash)

        return ratio * torch.exp(data_dash - diag_data)

    def forward(self, q, k, v):
        B, N, D = q.shape
        H = self.num_heads

        q = self.q_proj(q).view(B, N, H, -1).transpose(1, 2)
        k = self.k_proj(k).view(B, N, H, -1).transpose(1, 2)
        v = self.v_proj(v).view(B, N, H, -1).transpose(1, 2)

        # åˆ›å»ºæŠ•å½±çŸ©é˜µ
        projection_matrix = self.create_projection().to(q.device)

        # åº”ç”¨æ ¸ç‰¹å¾æ˜ å°„
        q_prime = self.kernel_feature_creator(q, projection_matrix)  # [B,H,N,M]
        k_prime = self.kernel_feature_creator(k, projection_matrix)

        # çº¿æ€§æ³¨æ„åŠ› O(n) å¤æ‚åº¦
        # out = (Q'(K'áµ€V)) / (Q'(K'áµ€1))
        kv = torch.einsum('...nm,...nd->...md', k_prime, v)  # [B,H,M,D/H]
        z = torch.einsum('...nm,...m->...n', q_prime,
                        k_prime.sum(dim=2))      # [B,H,N]
        out = torch.einsum('...nm,...md->...nd', q_prime, kv)
        out = out / z.unsqueeze(-1)

        out = out.transpose(1, 2).contiguous().view(B, N, D)
        out = self.o_proj(out)

        return out
```

#### 3.1.3 å®Œæ•´æ¨¡å‹æ¶æ„

```python
class ImprovedPepLand(nn.Module):
    """
    æ”¹è¿›çš„ PepLand æ¶æ„
    å…³é”®æ”¹è¿›:
    1. Graphormer æ›¿ä»£ HGT
    2. Performer åŠ é€Ÿæ³¨æ„åŠ›
    3. æ›´æ·±çš„ç½‘ç»œï¼ˆ12å±‚ï¼‰
    4. æ›´å¤§çš„éšè—ç»´åº¦ï¼ˆ512ï¼‰
    5. å±‚æ¬¡åŒ–æ± åŒ–
    """
    def __init__(
        self,
        atom_dim=42,
        bond_dim=14,
        pharm_dim=196,
        hidden_dim=512,      # ä»300æå‡åˆ°512
        num_layers=12,       # ä»5æå‡åˆ°12
        num_heads=8,
        dropout=0.1,
        use_performer=True
    ):
        super().__init__()

        # 1. è¾“å…¥æŠ•å½±
        self.atom_encoder = nn.Linear(atom_dim, hidden_dim)
        self.bond_encoder = nn.Linear(bond_dim, hidden_dim)
        self.pharm_encoder = nn.Linear(pharm_dim, hidden_dim)

        # 2. Graphormer å±‚
        if use_performer:
            self.layers = nn.ModuleList([
                GraphormerLayerWithPerformer(
                    hidden_dim, num_heads, dropout
                )
                for _ in range(num_layers)
            ])
        else:
            self.layers = nn.ModuleList([
                GraphormerLayer(hidden_dim, num_heads, dropout)
                for _ in range(num_layers)
            ])

        # 3. å±‚æ¬¡åŒ–æ± åŒ–
        self.hierarchical_pooling = HierarchicalPooling(
            hidden_dim, num_layers
        )

        # 4. å¤šå°ºåº¦èåˆ
        self.multi_scale_fusion = nn.Sequential(
            nn.Linear(hidden_dim * 3, hidden_dim * 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim)
        )

    def forward(self, batch_graph):
        # ç¼–ç èŠ‚ç‚¹å’Œè¾¹ç‰¹å¾
        x_atom = self.atom_encoder(batch_graph.ndata['atom_feat'])
        x_pharm = self.pharm_encoder(batch_graph.ndata['pharm_feat'])
        edge_attr = self.bond_encoder(batch_graph.edata['bond_feat'])

        # é¢„è®¡ç®—ç©ºé—´ç¼–ç ï¼ˆæœ€çŸ­è·¯å¾„ï¼‰
        spatial_pos = compute_shortest_path_distance(batch_graph)

        # è®¡ç®—åº¦æ•°ï¼ˆç”¨äºä¸­å¿ƒæ€§ç¼–ç ï¼‰
        in_degree = batch_graph.in_degrees()
        out_degree = batch_graph.out_degrees()

        # é€å±‚ä¼ é€’
        layer_outputs = []
        x = torch.cat([x_atom, x_pharm], dim=0)  # åˆå¹¶èŠ‚ç‚¹

        for layer in self.layers:
            x = layer(x, edge_attr, spatial_pos, in_degree, out_degree)
            layer_outputs.append(x)

        # å±‚æ¬¡åŒ–æ± åŒ–
        graph_repr = self.hierarchical_pooling(layer_outputs, batch_graph)

        return graph_repr


class HierarchicalPooling(nn.Module):
    """å±‚æ¬¡åŒ–å›¾æ± åŒ–"""
    def __init__(self, hidden_dim, num_layers):
        super().__init__()
        self.num_layers = num_layers

        # æ¯å±‚çš„æ± åŒ–æƒé‡
        self.layer_weights = nn.Parameter(
            torch.ones(num_layers) / num_layers
        )

        # ä¸åŒç²’åº¦çš„æ± åŒ–
        self.global_pool = GlobalAttentionPooling(hidden_dim)
        self.local_pool = SetTransformer(hidden_dim)

    def forward(self, layer_outputs, batch_graph):
        # 1. åŠ æƒèåˆæ‰€æœ‰å±‚çš„è¾“å‡º
        weighted_output = sum(
            w * out for w, out in zip(
                F.softmax(self.layer_weights, dim=0),
                layer_outputs
            )
        )

        # 2. å…¨å±€æ± åŒ–
        global_repr = self.global_pool(weighted_output, batch_graph)

        # 3. å±€éƒ¨æ± åŒ–ï¼ˆä¿ç•™ç»“æ„ä¿¡æ¯ï¼‰
        local_repr = self.local_pool(weighted_output, batch_graph)

        # 4. æœ€åä¸€å±‚çš„å‡å€¼æ± åŒ–
        mean_repr = dgl.mean_nodes(batch_graph, 'h')

        # 5. å¤šå°ºåº¦èåˆ
        graph_repr = torch.cat([global_repr, local_repr, mean_repr], dim=-1)

        return graph_repr
```

**é¢„æœŸæå‡**ï¼šç›¸æ¯”HGTæå‡ **8-12%**

**ç®—åŠ›å¼€é”€**ï¼šè®­ç»ƒæ—¶é—´å¢åŠ  **1.4-1.6x**ï¼ˆä½†Performerå‡å°‘äº†éƒ¨åˆ†å¼€é”€ï¼‰

---

### 3.2 å¼‚æ„å›¾å»ºæ¨¡å¢å¼º

#### 3.2.1 æ”¹è¿›çš„å¼‚æ„å›¾è¡¨ç¤º

```python
class EnhancedHeteroGraph(nn.Module):
    """
    å¢å¼ºçš„å¼‚æ„å›¾å»ºæ¨¡
    æ–°å¢:
    1. è™šæ‹ŸèŠ‚ç‚¹ï¼ˆè¿æ¥æ‰€æœ‰èŠ‚ç‚¹ï¼‰
    2. è¾¹-èŠ‚ç‚¹äº¤äº’
    3. å­å›¾æ± åŒ–
    """
    def __init__(self, hidden_dim):
        super().__init__()

        # è™šæ‹Ÿè¶…èŠ‚ç‚¹
        self.virtual_node_emb = nn.Parameter(
            torch.randn(1, hidden_dim)
        )

        # è™šæ‹ŸèŠ‚ç‚¹æ›´æ–°
        self.virtual_node_mlp = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

        # è¾¹-èŠ‚ç‚¹äº¤äº’
        self.edge_to_node = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, g, node_feat, edge_feat):
        batch_size = g.batch_size

        # 1. ä¸ºæ¯ä¸ªå›¾æ·»åŠ è™šæ‹ŸèŠ‚ç‚¹
        virtual_node = self.virtual_node_emb.repeat(batch_size, 1)

        # 2. è™šæ‹ŸèŠ‚ç‚¹èšåˆæ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯
        graph_mean = dgl.mean_nodes(g, 'h')
        virtual_node = self.virtual_node_mlp(
            torch.cat([virtual_node, graph_mean], dim=-1)
        )

        # 3. è™šæ‹ŸèŠ‚ç‚¹å¹¿æ’­å›æ‰€æœ‰èŠ‚ç‚¹
        node_feat = node_feat + virtual_node[g.batch_num_nodes()]

        # 4. è¾¹ç‰¹å¾èå…¥èŠ‚ç‚¹
        # edge_feat -> node_feat through message passing
        g.edata['e'] = self.edge_to_node(edge_feat)
        g.update_all(
            dgl.function.copy_e('e', 'm'),
            dgl.function.mean('m', 'edge_agg')
        )
        node_feat = node_feat + g.ndata['edge_agg']

        return node_feat, virtual_node
```

---

## 4. é¢„è®­ç»ƒç­–ç•¥ä¼˜åŒ–

### 4.1 å¤šä»»åŠ¡è‡ªç›‘ç£å­¦ä¹ 

#### 4.1.1 å¯¹æ¯”å­¦ä¹ ï¼ˆSimCLR for Graphsï¼‰

```python
class GraphContrastiveLearning(nn.Module):
    """
    å›¾å¯¹æ¯”å­¦ä¹ 
    åŸºäº SimCLR/MoCo çš„æ€æƒ³
    """
    def __init__(self, encoder, projection_dim=256, temperature=0.07):
        super().__init__()
        self.encoder = encoder
        self.temperature = temperature

        # æŠ•å½±å¤´
        self.projection = nn.Sequential(
            nn.Linear(encoder.hidden_dim, encoder.hidden_dim),
            nn.GELU(),
            nn.Linear(encoder.hidden_dim, projection_dim)
        )

        # Momentum encoder (MoCo)
        self.momentum_encoder = copy.deepcopy(encoder)
        self.momentum_projection = copy.deepcopy(self.projection)

        # å†»ç»“ momentum encoder
        for param in self.momentum_encoder.parameters():
            param.requires_grad = False
        for param in self.momentum_projection.parameters():
            param.requires_grad = False

        # é˜Ÿåˆ— (MoCo)
        self.register_buffer("queue", torch.randn(projection_dim, 65536))
        self.queue = F.normalize(self.queue, dim=0)
        self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))

    @torch.no_grad()
    def _momentum_update(self, momentum=0.999):
        """æ›´æ–° momentum encoder"""
        for param_q, param_k in zip(
            self.encoder.parameters(),
            self.momentum_encoder.parameters()
        ):
            param_k.data = param_k.data * momentum + param_q.data * (1. - momentum)

        for param_q, param_k in zip(
            self.projection.parameters(),
            self.momentum_projection.parameters()
        ):
            param_k.data = param_k.data * momentum + param_q.data * (1. - momentum)

    def forward(self, graph1, graph2):
        # ä¸¤ä¸ªå¢å¼ºè§†å›¾
        z1 = self.projection(self.encoder(graph1))  # [B, D]
        z1 = F.normalize(z1, dim=-1)

        # ä½¿ç”¨ momentum encoder
        with torch.no_grad():
            self._momentum_update()
            z2 = self.momentum_projection(
                self.momentum_encoder(graph2)
            )
            z2 = F.normalize(z2, dim=-1)

        # è®¡ç®—ç›¸ä¼¼åº¦
        # positive pairs: z1 vs z2
        l_pos = torch.einsum('nc,nc->n', [z1, z2]).unsqueeze(-1)  # [B, 1]

        # negative pairs: z1 vs queue
        l_neg = torch.einsum('nc,ck->nk', [z1, self.queue.clone().detach()])  # [B, K]

        # logits
        logits = torch.cat([l_pos, l_neg], dim=1)  # [B, 1+K]
        logits /= self.temperature

        # labels: æ­£æ ·æœ¬åœ¨ä½ç½®0
        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(logits.device)

        # InfoNCE loss
        loss = F.cross_entropy(logits, labels)

        # æ›´æ–°é˜Ÿåˆ—
        self._dequeue_and_enqueue(z2)

        return loss

    @torch.no_grad()
    def _dequeue_and_enqueue(self, keys):
        """æ›´æ–°é˜Ÿåˆ—"""
        batch_size = keys.shape[0]
        ptr = int(self.queue_ptr)

        # æ›¿æ¢é˜Ÿåˆ—ä¸­çš„é”®
        self.queue[:, ptr:ptr + batch_size] = keys.T
        ptr = (ptr + batch_size) % self.queue.shape[1]

        self.queue_ptr[0] = ptr


# æ•°æ®å¢å¼ºç­–ç•¥
class GraphAugmentation:
    """å›¾æ•°æ®å¢å¼º"""

    @staticmethod
    def node_dropping(graph, drop_ratio=0.1):
        """éšæœºåˆ é™¤èŠ‚ç‚¹"""
        num_nodes = graph.number_of_nodes()
        drop_num = int(num_nodes * drop_ratio)
        keep_nodes = torch.randperm(num_nodes)[:-drop_num]
        return graph.subgraph(keep_nodes)

    @staticmethod
    def edge_perturbation(graph, perturb_ratio=0.1):
        """è¾¹æ‰°åŠ¨ï¼šéšæœºåˆ é™¤/æ·»åŠ è¾¹"""
        src, dst = graph.edges()
        num_edges = len(src)

        # åˆ é™¤è¾¹
        drop_num = int(num_edges * perturb_ratio)
        keep_edges = torch.randperm(num_edges)[:-drop_num]

        new_graph = dgl.graph((src[keep_edges], dst[keep_edges]))
        new_graph.ndata['feat'] = graph.ndata['feat']

        return new_graph

    @staticmethod
    def subgraph_sampling(graph, sample_ratio=0.7):
        """å­å›¾é‡‡æ ·"""
        num_nodes = graph.number_of_nodes()
        sample_num = int(num_nodes * sample_ratio)
        sampled_nodes = torch.randperm(num_nodes)[:sample_num]
        return graph.subgraph(sampled_nodes)

    @staticmethod
    def feature_masking(graph, mask_ratio=0.15):
        """ç‰¹å¾é®è”½"""
        new_graph = graph.clone()
        num_nodes = graph.number_of_nodes()
        mask_num = int(num_nodes * mask_ratio)
        mask_nodes = torch.randperm(num_nodes)[:mask_num]
        new_graph.ndata['feat'][mask_nodes] = 0
        return new_graph

    @staticmethod
    def compose_augmentations(graph):
        """ç»„åˆå¤šç§å¢å¼º"""
        # éšæœºé€‰æ‹©2-3ç§å¢å¼ºæ–¹å¼
        aug_list = [
            GraphAugmentation.node_dropping,
            GraphAugmentation.edge_perturbation,
            GraphAugmentation.feature_masking,
            GraphAugmentation.subgraph_sampling
        ]

        num_augs = random.randint(2, 3)
        selected_augs = random.sample(aug_list, num_augs)

        aug_graph = graph
        for aug_fn in selected_augs:
            aug_graph = aug_fn(aug_graph)

        return aug_graph
```

#### 4.1.2 ç”Ÿæˆå¼é¢„è®­ç»ƒä»»åŠ¡

```python
class GenerativePretraining(nn.Module):
    """
    ç”Ÿæˆå¼é¢„è®­ç»ƒ
    ä»»åŠ¡åŒ…æ‹¬:
    1. ç‰‡æ®µé‡å»º
    2. è¾¹é‡å»º
    3. å›¾å±æ€§é¢„æµ‹
    """
    def __init__(self, encoder, hidden_dim):
        super().__init__()
        self.encoder = encoder

        # ç‰‡æ®µé‡å»ºå¤´
        self.fragment_decoder = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim * 2),
            nn.GELU(),
            nn.Linear(hidden_dim * 2, vocab_size)
        )

        # è¾¹é‡å»ºå¤´
        self.edge_decoder = nn.Bilinear(hidden_dim, hidden_dim, num_edge_types)

        # å›¾å±æ€§é¢„æµ‹å¤´ï¼ˆå¤šä»»åŠ¡ï¼‰
        self.property_predictor = nn.ModuleDict({
            'num_atoms': nn.Linear(hidden_dim, 1),
            'num_rings': nn.Linear(hidden_dim, 10),
            'molecular_weight': nn.Linear(hidden_dim, 1),
            'is_cyclic': nn.Linear(hidden_dim, 2)
        })

    def forward(self, graph, masked_nodes, masked_edges):
        # ç¼–ç 
        node_repr = self.encoder(graph)
        graph_repr = dgl.mean_nodes(graph, 'h')

        # ä»»åŠ¡1: ç‰‡æ®µé‡å»º
        fragment_loss = F.cross_entropy(
            self.fragment_decoder(node_repr[masked_nodes]),
            graph.ndata['fragment_label'][masked_nodes]
        )

        # ä»»åŠ¡2: è¾¹é‡å»º
        src, dst = masked_edges
        edge_loss = F.cross_entropy(
            self.edge_decoder(node_repr[src], node_repr[dst]),
            graph.edata['edge_label'][masked_edges]
        )

        # ä»»åŠ¡3: å›¾å±æ€§é¢„æµ‹
        property_losses = {}
        for prop_name, predictor in self.property_predictor.items():
            pred = predictor(graph_repr)
            target = graph.graph_attr[prop_name]
            if prop_name in ['num_atoms', 'molecular_weight']:
                property_losses[prop_name] = F.mse_loss(pred, target)
            else:
                property_losses[prop_name] = F.cross_entropy(pred, target)

        # æ€»æŸå¤±
        total_loss = fragment_loss + edge_loss + sum(property_losses.values())

        return total_loss, {
            'fragment': fragment_loss,
            'edge': edge_loss,
            **property_losses
        }
```

#### 4.1.3 è¯¾ç¨‹å­¦ä¹ 

```python
class CurriculumLearning:
    """
    è¯¾ç¨‹å­¦ä¹ ï¼šä»ç®€å•åˆ°å¤æ‚
    """
    def __init__(self, dataset, difficulty_metric='num_atoms'):
        self.dataset = dataset
        self.difficulty_metric = difficulty_metric

        # æŒ‰éš¾åº¦æ’åº
        self.sorted_indices = self._sort_by_difficulty()
        self.current_stage = 0
        self.num_stages = 5

    def _sort_by_difficulty(self):
        """æŒ‰éš¾åº¦æŒ‡æ ‡æ’åºæ•°æ®"""
        difficulties = []
        for data in self.dataset:
            if self.difficulty_metric == 'num_atoms':
                diff = data.num_nodes()
            elif self.difficulty_metric == 'num_fragments':
                diff = len(data.fragments)
            elif self.difficulty_metric == 'complexity':
                # å¤æ‚åº¦ = èŠ‚ç‚¹æ•° + è¾¹æ•° + ç¯æ•°
                diff = data.num_nodes() + data.num_edges() + data.num_rings
            difficulties.append(diff)

        return np.argsort(difficulties)

    def get_curriculum_subset(self, epoch, total_epochs):
        """è·å–å½“å‰è¯¾ç¨‹é˜¶æ®µçš„æ•°æ®å­é›†"""
        # è®¡ç®—å½“å‰é˜¶æ®µ
        stage = min(int(epoch / total_epochs * self.num_stages),
                   self.num_stages - 1)

        # é€æ¸å¢åŠ æ•°æ®éš¾åº¦
        # é˜¶æ®µ0: æœ€ç®€å•çš„20%
        # é˜¶æ®µ1: æœ€ç®€å•çš„40%
        # ...
        # é˜¶æ®µ4: å…¨éƒ¨æ•°æ®
        ratio = (stage + 1) / self.num_stages
        cutoff = int(len(self.sorted_indices) * ratio)

        current_indices = self.sorted_indices[:cutoff]

        # åœ¨å½“å‰å­é›†å†…éšæœºæ‰“ä¹±
        np.random.shuffle(current_indices)

        return Subset(self.dataset, current_indices)
```

**é¢„æœŸæå‡**ï¼š
- å¯¹æ¯”å­¦ä¹ : **+8-10%**
- ç”Ÿæˆå¼ä»»åŠ¡: **+3-5%**
- è¯¾ç¨‹å­¦ä¹ : **+2-4%**

**æ€»è®¡**: **+13-19%**

---

### 4.2 é¢„è®­ç»ƒé…ç½®ä¼˜åŒ–

```yaml
# æ”¹è¿›çš„é¢„è®­ç»ƒé…ç½®
pretraining:
  # æ¨¡å‹
  model: ImprovedPepLand
  hidden_dim: 512          # ä»300æå‡
  num_layers: 12           # ä»5æå‡
  num_heads: 8
  use_performer: true

  # è®­ç»ƒ
  batch_size: 256          # é€‚å½“å‡å°ï¼ˆæ¨¡å‹æ›´å¤§ï¼‰
  epochs: 100              # å¢åŠ epochæ•°
  warmup_epochs: 10
  lr: 0.0001               # æ›´å°çš„å­¦ä¹ ç‡
  weight_decay: 0.01

  # é¢„è®­ç»ƒä»»åŠ¡æƒé‡
  task_weights:
    masked_prediction: 1.0
    contrastive: 0.5
    generative: 0.3

  # å¯¹æ¯”å­¦ä¹ 
  contrastive:
    temperature: 0.07
    queue_size: 65536
    momentum: 0.999
    augmentation:
      - node_dropping: 0.1
      - edge_perturb: 0.1
      - feature_mask: 0.15

  # è¯¾ç¨‹å­¦ä¹ 
  curriculum:
    enabled: true
    num_stages: 5
    difficulty_metric: complexity

  # ä¼˜åŒ–å™¨
  optimizer:
    type: AdamW
    betas: [0.9, 0.999]
    eps: 1e-8

  # å­¦ä¹ ç‡è°ƒåº¦
  lr_scheduler:
    type: cosine_with_warmup
    warmup_steps: 10000
    min_lr: 1e-6

  # æ··åˆç²¾åº¦
  mixed_precision: true

  # æ¢¯åº¦è£å‰ª
  grad_clip: 1.0
```

---

## 5. ç‰¹å¾å·¥ç¨‹å¢å¼º

### 5.1 3Dæ„è±¡ä¿¡æ¯é›†æˆ

```python
class Conformer3DEncoder(nn.Module):
    """
    3Dæ„è±¡ç¼–ç å™¨
    ä½¿ç”¨ RDKit ç”Ÿæˆ3Dæ„è±¡
    """
    def __init__(self, hidden_dim):
        super().__init__()

        # 3Dåæ ‡ç¼–ç 
        self.coord_encoder = nn.Sequential(
            nn.Linear(3, hidden_dim // 2),
            nn.GELU(),
            nn.Linear(hidden_dim // 2, hidden_dim)
        )

        # è·ç¦»çŸ©é˜µç¼–ç 
        self.distance_encoder = nn.Sequential(
            nn.Linear(1, hidden_dim // 4),
            nn.GELU(),
            nn.Linear(hidden_dim // 4, hidden_dim // 2)
        )

        # è§’åº¦ç¼–ç 
        self.angle_encoder = nn.Sequential(
            nn.Linear(1, hidden_dim // 4),
            nn.GELU(),
            nn.Linear(hidden_dim // 4, hidden_dim // 2)
        )

        # èåˆå±‚
        self.fusion = nn.Linear(hidden_dim * 2, hidden_dim)

    def forward(self, coords, distance_matrix, angles):
        """
        Args:
            coords: [N, 3] åŸå­3Dåæ ‡
            distance_matrix: [N, N] åŸå­é—´è·ç¦»
            angles: [N, N, N] é”®è§’
        """
        # åæ ‡ç¼–ç 
        coord_feat = self.coord_encoder(coords)  # [N, D]

        # è·ç¦»ç¼–ç 
        dist_feat = self.distance_encoder(
            distance_matrix.unsqueeze(-1)
        )  # [N, N, D/2]
        dist_feat = dist_feat.mean(dim=1)  # [N, D/2]

        # è§’åº¦ç¼–ç 
        angle_feat = self.angle_encoder(
            angles.unsqueeze(-1)
        )  # [N, N, N, D/2]
        angle_feat = angle_feat.mean(dim=[1, 2])  # [N, D/2]

        # èåˆ
        combined = torch.cat([
            coord_feat,
            dist_feat,
            angle_feat
        ], dim=-1)

        output = self.fusion(combined)

        return output


def generate_3d_features(smiles_list):
    """
    ä¸ºSMILESåˆ—è¡¨ç”Ÿæˆ3Dç‰¹å¾
    ä½¿ç”¨RDKitçš„ETKDGç®—æ³•
    """
    features_list = []

    for smiles in smiles_list:
        mol = Chem.MolFromSmiles(smiles)

        # æ·»åŠ æ°¢åŸå­
        mol = Chem.AddHs(mol)

        # ç”Ÿæˆ3Dæ„è±¡
        AllChem.EmbedMolecule(mol, randomSeed=42)
        AllChem.MMFFOptimizeMolecule(mol)

        # æå–åæ ‡
        conf = mol.GetConformer()
        coords = np.array([
            list(conf.GetAtomPosition(i))
            for i in range(mol.GetNumAtoms())
        ])

        # è®¡ç®—è·ç¦»çŸ©é˜µ
        distance_matrix = distance_matrix_from_coords(coords)

        # è®¡ç®—é”®è§’
        angles = compute_bond_angles(mol)

        features_list.append({
            'coords': coords,
            'distance_matrix': distance_matrix,
            'angles': angles
        })

    return features_list
```

### 5.2 ç‰©ç†åŒ–å­¦æ€§è´¨ç‰¹å¾

```python
class PhysicoChemicalFeatures(nn.Module):
    """
    ç‰©ç†åŒ–å­¦æ€§è´¨ç‰¹å¾
    """
    def __init__(self, hidden_dim):
        super().__init__()

        # åˆ†å­æè¿°ç¬¦ç»´åº¦
        descriptor_dim = 200  # RDKitçš„200ä¸ªæè¿°ç¬¦

        self.descriptor_encoder = nn.Sequential(
            nn.Linear(descriptor_dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim)
        )

    def forward(self, smiles):
        """æå–å¹¶ç¼–ç ç‰©ç†åŒ–å­¦æè¿°ç¬¦"""
        from rdkit.Chem import Descriptors
        from rdkit.ML.Descriptors import MoleculeDescriptors

        # è®¡ç®—æ‰€æœ‰RDKitæè¿°ç¬¦
        descriptor_names = [x[0] for x in Descriptors._descList]
        calculator = MoleculeDescriptors.MolecularDescriptorCalculator(
            descriptor_names
        )

        mol = Chem.MolFromSmiles(smiles)
        descriptors = np.array(calculator.CalcDescriptors(mol))

        # æ ‡å‡†åŒ–
        descriptors = (descriptors - self.mean) / (self.std + 1e-8)

        # ç¼–ç 
        descriptors_tensor = torch.FloatTensor(descriptors)
        encoded = self.descriptor_encoder(descriptors_tensor)

        return encoded
```

### 5.3 åºåˆ—ä¿¡æ¯ç¼–ç 

```python
class SequenceEncoder(nn.Module):
    """
    æ°¨åŸºé…¸åºåˆ—ç¼–ç å™¨
    ä½¿ç”¨ESM-2ç­‰è›‹ç™½è´¨è¯­è¨€æ¨¡å‹
    """
    def __init__(self, hidden_dim):
        super().__init__()

        # åŠ è½½ESM-2æ¨¡å‹
        import esm
        self.esm_model, self.alphabet = esm.pretrained.esm2_t33_650M_UR50D()
        self.batch_converter = self.alphabet.get_batch_converter()

        # æŠ•å½±å±‚
        self.projection = nn.Linear(1280, hidden_dim)  # ESM-2ç»´åº¦æ˜¯1280

    def forward(self, sequences):
        """
        Args:
            sequences: List of amino acid sequences
        Returns:
            sequence_embeddings: [B, L, D]
        """
        # å‡†å¤‡æ•°æ®
        data = [(f"seq_{i}", seq) for i, seq in enumerate(sequences)]
        batch_labels, batch_strs, batch_tokens = self.batch_converter(data)

        # ESM-2ç¼–ç 
        with torch.no_grad():
            results = self.esm_model(
                batch_tokens,
                repr_layers=[33],
                return_contacts=False
            )

        # è·å–è¡¨ç¤º
        embeddings = results["representations"][33]  # [B, L, 1280]

        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        embeddings = self.projection(embeddings)  # [B, L, D]

        return embeddings
```

**é¢„æœŸæå‡**ï¼š
- 3Dæ„è±¡: **+5-8%** (åœ¨ç»“åˆã€æº¶è§£åº¦ä»»åŠ¡ä¸Š)
- ç‰©åŒ–æ€§è´¨: **+3-5%**
- åºåˆ—ä¿¡æ¯: **+4-6%** (å¯¹è§„èŒƒæ°¨åŸºé…¸è‚½æ®µ)

---

## 6. è®­ç»ƒæŠ€å·§ä¼˜åŒ–

### 6.1 é«˜çº§ä¼˜åŒ–å™¨

```python
# ä½¿ç”¨ AdamW + å­¦ä¹ ç‡é¢„çƒ­ + Cosineè¡°å‡
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts

class OptimizerWithScheduler:
    def __init__(self, model, config):
        # AdamWä¼˜åŒ–å™¨
        self.optimizer = AdamW(
            model.parameters(),
            lr=config.lr,
            betas=(0.9, 0.999),
            eps=1e-8,
            weight_decay=config.weight_decay
        )

        # Cosineé€€ç« + é‡å¯
        self.scheduler = CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=config.T_0,        # åˆå§‹å‘¨æœŸ
            T_mult=config.T_mult,  # å‘¨æœŸå€å¢å› å­
            eta_min=config.min_lr  # æœ€å°å­¦ä¹ ç‡
        )

        self.warmup_steps = config.warmup_steps
        self.base_lr = config.lr
        self.current_step = 0

    def step(self):
        # å­¦ä¹ ç‡é¢„çƒ­
        if self.current_step < self.warmup_steps:
            lr = self.base_lr * (self.current_step / self.warmup_steps)
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = lr

        self.optimizer.step()
        self.scheduler.step()
        self.current_step += 1
```

### 6.2 æ­£åˆ™åŒ–æŠ€æœ¯

```python
# 1. Dropout
model = ImprovedPepLand(dropout=0.1)

# 2. DropPath (Stochastic Depth)
class DropPath(nn.Module):
    def __init__(self, drop_prob=0.1):
        super().__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        if not self.training or self.drop_prob == 0.:
            return x
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()
        output = x.div(keep_prob) * random_tensor
        return output

# 3. Label Smoothing
class LabelSmoothingCrossEntropy(nn.Module):
    def __init__(self, smoothing=0.1):
        super().__init__()
        self.smoothing = smoothing

    def forward(self, pred, target):
        n_class = pred.size(-1)
        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)
        smooth_one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class
        loss = -smooth_one_hot * F.log_softmax(pred, dim=-1)
        return loss.sum(dim=-1).mean()

# 4. Mixup (for graph)
def graph_mixup(graph1, graph2, alpha=0.2):
    lam = np.random.beta(alpha, alpha)
    mixed_graph = dgl.batch([graph1, graph2])
    mixed_graph.ndata['feat'] = lam * graph1.ndata['feat'] + \
                                (1 - lam) * graph2.ndata['feat']
    return mixed_graph, lam
```

### 6.3 æ¢¯åº¦ä¼˜åŒ–

```python
# 1. æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 2. æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model(batch)
    loss = loss / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()

# 3. æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
for batch in dataloader:
    with autocast():
        loss = model(batch)

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad()
```

### 6.4 Early Stopping & Model Checkpoint

```python
class EarlyStoppingWithCheckpoint:
    def __init__(self, patience=10, delta=0.001):
        self.patience = patience
        self.delta = delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.best_model = None

    def __call__(self, val_metric, model):
        score = val_metric

        if self.best_score is None:
            self.best_score = score
            self.best_model = copy.deepcopy(model.state_dict())
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.best_model = copy.deepcopy(model.state_dict())
            self.counter = 0

        return self.early_stop
```

---

## 7. ä¸‹æ¸¸ä»»åŠ¡é€‚é…

### 7.1 é«˜æ•ˆå¾®è°ƒç­–ç•¥

#### 7.1.1 Adapterå¾®è°ƒ

```python
class Adapter(nn.Module):
    """
    Adapteræ¨¡å— - åªè®­ç»ƒå°‘é‡å‚æ•°
    å‚æ•°é‡: 0.5-2% çš„åŸæ¨¡å‹
    """
    def __init__(self, hidden_dim, adapter_dim=64):
        super().__init__()
        self.down_project = nn.Linear(hidden_dim, adapter_dim)
        self.activation = nn.GELU()
        self.up_project = nn.Linear(adapter_dim, hidden_dim)

        # åˆå§‹åŒ–ä¸ºæ¥è¿‘æ’ç­‰æ˜ å°„
        nn.init.zeros_(self.up_project.weight)
        nn.init.zeros_(self.up_project.bias)

    def forward(self, x):
        residual = x
        x = self.down_project(x)
        x = self.activation(x)
        x = self.up_project(x)
        return x + residual


class ModelWithAdapters(nn.Module):
    """åœ¨æ¯ä¸ªTransformerå±‚åæ·»åŠ Adapter"""
    def __init__(self, pretrained_model):
        super().__init__()
        self.model = pretrained_model

        # å†»ç»“é¢„è®­ç»ƒæ¨¡å‹
        for param in self.model.parameters():
            param.requires_grad = False

        # æ·»åŠ Adapters
        self.adapters = nn.ModuleList([
            Adapter(self.model.hidden_dim)
            for _ in range(self.model.num_layers)
        ])

    def forward(self, x):
        for i, layer in enumerate(self.model.layers):
            x = layer(x)
            x = self.adapters[i](x)  # Adapter
        return x
```

#### 7.1.2 LoRAå¾®è°ƒ

```python
class LoRALayer(nn.Module):
    """
    LoRA (Low-Rank Adaptation)
    åªè®­ç»ƒä½ç§©åˆ†è§£çŸ©é˜µï¼Œå‚æ•°é‡æ›´å°‘
    """
    def __init__(self, in_features, out_features, rank=8, alpha=16):
        super().__init__()
        self.rank = rank
        self.alpha = alpha

        # ä½ç§©åˆ†è§£: W + Î”W = W + BA
        self.lora_A = nn.Parameter(torch.randn(in_features, rank))
        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))

        self.scaling = alpha / rank

    def forward(self, x, original_weight):
        # åŸå§‹æƒé‡ï¼ˆå†»ç»“ï¼‰
        out = F.linear(x, original_weight)

        # LoRAå¢é‡
        lora_out = (x @ self.lora_A @ self.lora_B) * self.scaling

        return out + lora_out


class ModelWithLoRA(nn.Module):
    """åœ¨æ³¨æ„åŠ›å±‚åº”ç”¨LoRA"""
    def __init__(self, pretrained_model, rank=8):
        super().__init__()
        self.model = pretrained_model

        # å†»ç»“é¢„è®­ç»ƒæ¨¡å‹
        for param in self.model.parameters():
            param.requires_grad = False

        # ä¸ºQueryå’ŒValueæŠ•å½±æ·»åŠ LoRA
        self.lora_layers = nn.ModuleDict()
        for i, layer in enumerate(self.model.layers):
            self.lora_layers[f'layer_{i}_q'] = LoRALayer(
                layer.hidden_dim, layer.hidden_dim, rank
            )
            self.lora_layers[f'layer_{i}_v'] = LoRALayer(
                layer.hidden_dim, layer.hidden_dim, rank
            )
```

### 7.2 å¤šä»»åŠ¡å­¦ä¹ 

```python
class MultiTaskLearning(nn.Module):
    """
    å¤šä»»åŠ¡å­¦ä¹ æ¡†æ¶
    å…±äº«ç¼–ç å™¨ + ä»»åŠ¡ç‰¹å®šå¤´
    """
    def __init__(self, encoder, tasks):
        super().__init__()
        self.encoder = encoder

        # ä»»åŠ¡ç‰¹å®šçš„å¤´
        self.task_heads = nn.ModuleDict({
            'binding': nn.Sequential(
                nn.Linear(encoder.hidden_dim, 256),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(256, 1)
            ),
            'cpp': nn.Sequential(
                nn.Linear(encoder.hidden_dim, 256),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(256, 2)  # äºŒåˆ†ç±»
            ),
            'solubility': nn.Sequential(
                nn.Linear(encoder.hidden_dim, 256),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(256, 1)
            ),
            'synthesizability': nn.Sequential(
                nn.Linear(encoder.hidden_dim, 256),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(256, 2)
            )
        })

        # ä»»åŠ¡æƒé‡ï¼ˆå¯å­¦ä¹ ï¼‰
        self.task_weights = nn.Parameter(torch.ones(len(tasks)))

    def forward(self, batch, task_name=None):
        # å…±äº«ç¼–ç 
        graph_repr = self.encoder(batch)

        if task_name:
            # å•ä»»åŠ¡
            return self.task_heads[task_name](graph_repr)
        else:
            # å¤šä»»åŠ¡
            outputs = {}
            for task, head in self.task_heads.items():
                outputs[task] = head(graph_repr)
            return outputs

    def compute_loss(self, outputs, targets):
        """åŠ æƒå¤šä»»åŠ¡æŸå¤±"""
        losses = {}
        for task in outputs.keys():
            if task in targets:
                pred = outputs[task]
                target = targets[task]

                if task in ['binding', 'solubility']:
                    losses[task] = F.mse_loss(pred, target)
                else:
                    losses[task] = F.cross_entropy(pred, target)

        # ä½¿ç”¨å¯å­¦ä¹ æƒé‡
        weighted_loss = sum(
            torch.exp(-self.task_weights[i]) * loss + self.task_weights[i]
            for i, loss in enumerate(losses.values())
        )

        return weighted_loss, losses
```

### 7.3 Few-Shot Learning

```python
class ProtoNet(nn.Module):
    """
    Prototypical Networks for Few-Shot Learning
    ç”¨äºå°æ ·æœ¬ä»»åŠ¡
    """
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder

    def forward(self, support_set, support_labels, query_set):
        """
        Args:
            support_set: [N_support, ...]
            support_labels: [N_support]
            query_set: [N_query, ...]
        """
        # ç¼–ç supportå’Œquery
        support_embeddings = self.encoder(support_set)  # [N_s, D]
        query_embeddings = self.encoder(query_set)      # [N_q, D]

        # è®¡ç®—æ¯ä¸ªç±»çš„åŸå‹ï¼ˆprototypeï¼‰
        unique_labels = torch.unique(support_labels)
        prototypes = []
        for label in unique_labels:
            mask = support_labels == label
            prototype = support_embeddings[mask].mean(dim=0)
            prototypes.append(prototype)

        prototypes = torch.stack(prototypes)  # [N_classes, D]

        # è®¡ç®—queryåˆ°æ¯ä¸ªåŸå‹çš„è·ç¦»
        distances = torch.cdist(query_embeddings, prototypes)  # [N_q, N_classes]

        # ä½¿ç”¨è´Ÿè·ç¦»ä½œä¸ºlogits
        logits = -distances

        return logits


# è®­ç»ƒå¾ªç¯
def train_few_shot(model, support_loader, query_loader, n_way=5, k_shot=5):
    for support_batch, query_batch in zip(support_loader, query_loader):
        # é‡‡æ ·N-way K-shot
        support_set, support_labels = sample_episode(
            support_batch, n_way, k_shot
        )

        logits = model(support_set, support_labels, query_batch.x)
        loss = F.cross_entropy(logits, query_batch.y)

        loss.backward()
        optimizer.step()
```

---

## 8. å®æ–½è·¯çº¿å›¾

### 8.1 ä¸‰é˜¶æ®µå®æ–½è®¡åˆ’

#### é˜¶æ®µ1: åŸºç¡€æ”¹è¿› (Week 1-2)
**ç›®æ ‡**: å¿«é€Ÿå®ç°ä½æˆæœ¬æ”¹è¿›ï¼ŒéªŒè¯æ–¹å‘

```python
priorities = [
    "1. æ›¿æ¢ä¸ºGraphormeræ¶æ„",          # é¢„æœŸ +8%
    "2. å¢åŠ æ¨¡å‹æ·±åº¦åˆ°12å±‚",             # é¢„æœŸ +3%
    "3. æå‡hidden_dimåˆ°512",           # é¢„æœŸ +2%
    "4. å®ç°å¯¹æ¯”å­¦ä¹ ",                   # é¢„æœŸ +8%
    "5. æ”¹è¿›æ•°æ®å¢å¼º",                   # é¢„æœŸ +3%
]

# éªŒè¯å®éªŒ
- åœ¨10%æ•°æ®ä¸Šå¿«é€ŸéªŒè¯
- å¯¹æ¯”åŸºçº¿æ€§èƒ½
- å†³å®šæ˜¯å¦ç»§ç»­
```

**æ—¶é—´**: 5-7å¤©
**ç®—åŠ›**: 2Ã—4090
**é¢„æœŸæå‡**: +15-20%

#### é˜¶æ®µ2: æ·±åº¦ä¼˜åŒ– (Week 3-4)
**ç›®æ ‡**: å®ç°é«˜çº§ç‰¹å¾å’Œè®­ç»ƒç­–ç•¥

```python
priorities = [
    "1. é›†æˆ3Dæ„è±¡ç‰¹å¾",               # é¢„æœŸ +5%
    "2. æ·»åŠ ç‰©åŒ–æ€§è´¨ç‰¹å¾",             # é¢„æœŸ +3%
    "3. å®ç°PerformeråŠ é€Ÿ",            # å‡å°‘è®­ç»ƒæ—¶é—´30%
    "4. è¯¾ç¨‹å­¦ä¹ ",                     # é¢„æœŸ +3%
    "5. ä¼˜åŒ–è®­ç»ƒæµç¨‹",                 # é¢„æœŸ +2%
]
```

**æ—¶é—´**: 10-14å¤©
**ç®—åŠ›**: 4Ã—4090
**é¢„æœŸæå‡**: +10-13% (ç´¯è®¡ +25-33%)

#### é˜¶æ®µ3: å¾®è°ƒå’Œé›†æˆ (Week 5-6)
**ç›®æ ‡**: é’ˆå¯¹ä¸‹æ¸¸ä»»åŠ¡ä¼˜åŒ–ï¼Œé›†æˆæå‡

```python
priorities = [
    "1. Adapter/LoRAå¾®è°ƒ",             # é¢„æœŸ +3%
    "2. å¤šä»»åŠ¡å­¦ä¹ ",                   # é¢„æœŸ +4%
    "3. é›†æˆå­¦ä¹ (3-5ä¸ªæ¨¡å‹)",          # é¢„æœŸ +3%
    "4. è¶…å‚æ•°æœç´¢",                   # é¢„æœŸ +2%
    "5. æµ‹è¯•æ—¶å¢å¼º(TTA)",              # é¢„æœŸ +2%
]
```

**æ—¶é—´**: 10-14å¤©
**ç®—åŠ›**: 4Ã—4090
**é¢„æœŸæå‡**: +10-14% (ç´¯è®¡ +35-47%)

### 8.2 è¯¦ç»†æ—¶é—´è¡¨

| å‘¨æ¬¡ | ä»»åŠ¡ | ç®—åŠ› | è¾“å‡º |
|------|------|------|------|
| W1 | æ¶æ„æ”¹è¿› + å¯¹æ¯”å­¦ä¹  | 2Ã—4090 | baseline+15% |
| W2 | å®Œæ•´è®­ç»ƒ + æ¶ˆèå®éªŒ | 4Ã—4090 | éªŒè¯æŠ¥å‘Š |
| W3 | 3Dç‰¹å¾ + è¯¾ç¨‹å­¦ä¹  | 4Ã—4090 | baseline+28% |
| W4 | ä¼˜åŒ–è®­ç»ƒæµç¨‹ | 4Ã—4090 | æœ€ä¼˜é…ç½® |
| W5 | ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ | 2Ã—4090 | å„ä»»åŠ¡æ€§èƒ½ |
| W6 | é›†æˆå­¦ä¹  + æœ€ç»ˆè¯„ä¼° | 4Ã—4090 | **æœ€ç»ˆæ¨¡å‹** |

### 8.3 ç®—åŠ›é¢„ç®—

```python
# æ€»ç®—åŠ›é¢„ç®—ï¼ˆ4Ã—RTX 4090ï¼‰
total_compute = 4 * 165 TFLOPS * 6 weeks

# åˆ†é…
phase1_hours = 80 hours  # åŸºç¡€æ”¹è¿›
phase2_hours = 120 hours # æ·±åº¦ä¼˜åŒ–
phase3_hours = 100 hours # å¾®è°ƒé›†æˆ

total_hours = 300 hours

# æˆæœ¬
electricity_cost = 300h * 0.55kW * 4å¡ * Â¥1/kWh = Â¥660
```

### 8.4 ä»£ç å®ç°è®¡åˆ’

```
pepland_improved/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ graphormer.py          # Graphormerå®ç°
â”‚   â”œâ”€â”€ performer.py            # Performeræ³¨æ„åŠ›
â”‚   â”œâ”€â”€ hierarchical_pool.py   # å±‚æ¬¡åŒ–æ± åŒ–
â”‚   â””â”€â”€ improved_pepland.py    # å®Œæ•´æ¨¡å‹
â”œâ”€â”€ pretraining/
â”‚   â”œâ”€â”€ contrastive.py         # å¯¹æ¯”å­¦ä¹ 
â”‚   â”œâ”€â”€ generative.py          # ç”Ÿæˆå¼ä»»åŠ¡
â”‚   â”œâ”€â”€ curriculum.py          # è¯¾ç¨‹å­¦ä¹ 
â”‚   â””â”€â”€ augmentation.py        # æ•°æ®å¢å¼º
â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ conformer_3d.py        # 3Dæ„è±¡
â”‚   â”œâ”€â”€ physicochemical.py     # ç‰©åŒ–æ€§è´¨
â”‚   â””â”€â”€ sequence.py            # åºåˆ—ç¼–ç 
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ optimizer.py           # ä¼˜åŒ–å™¨é…ç½®
â”‚   â”œâ”€â”€ scheduler.py           # å­¦ä¹ ç‡è°ƒåº¦
â”‚   â”œâ”€â”€ regularization.py      # æ­£åˆ™åŒ–
â”‚   â””â”€â”€ trainer.py             # è®­ç»ƒå¾ªç¯
â”œâ”€â”€ finetuning/
â”‚   â”œâ”€â”€ adapter.py             # Adapterå¾®è°ƒ
â”‚   â”œâ”€â”€ lora.py                # LoRAå¾®è°ƒ
â”‚   â”œâ”€â”€ multitask.py           # å¤šä»»åŠ¡å­¦ä¹ 
â”‚   â””â”€â”€ fewshot.py             # Few-shotå­¦ä¹ 
â””â”€â”€ utils/
    â”œâ”€â”€ metrics.py             # è¯„ä¼°æŒ‡æ ‡
    â””â”€â”€ visualization.py       # å¯è§†åŒ–
```

---

## 9. é¢„æœŸæ€§èƒ½æå‡

### 9.1 å®šé‡é¢„æµ‹

åŸºäºæ–‡çŒ®å’Œç»éªŒä¼°ç®—ï¼Œå„æ”¹è¿›çš„ç´¯è®¡æ•ˆæœï¼š

| ä»»åŠ¡ | PepLandåŸºçº¿ | é¢„æœŸæå‡ | æ”¹è¿›å | ç›®æ ‡ |
|------|-------------|----------|--------|------|
| **Binding (Pearson)** | 0.67 | +40% | **0.94** | âœ… |
| **CPP (AUC)** | 0.77 | +25% | **0.96** | âœ… |
| **Solubility (RMSE)** | 1.35 | -30% | **0.95** | âœ… |
| **Synthesis (Acc)** | 0.72 | +35% | **0.97** | âœ… |

### 9.2 æ”¹è¿›æ¥æºåˆ†è§£

```python
æ€»æå‡åˆ†è§£ï¼š
â”œâ”€â”€ æ¨¡å‹æ¶æ„ (Graphormer + Performer):  +10-12%
â”œâ”€â”€ å¯¹æ¯”å­¦ä¹ :                           +8-10%
â”œâ”€â”€ 3Dæ„è±¡ç‰¹å¾:                        +5-7%
â”œâ”€â”€ ç”Ÿæˆå¼é¢„è®­ç»ƒ:                      +3-5%
â”œâ”€â”€ è¯¾ç¨‹å­¦ä¹ :                          +2-4%
â”œâ”€â”€ é«˜æ•ˆå¾®è°ƒ:                          +5-7%
â”œâ”€â”€ å¤šä»»åŠ¡å­¦ä¹ :                        +3-5%
â”œâ”€â”€ é›†æˆå­¦ä¹ :                          +3-5%
â””â”€â”€ å…¶ä»–ä¼˜åŒ–:                          +3-5%

ä¿å®ˆä¼°è®¡: +35-45%
ä¹è§‚ä¼°è®¡: +50-60%
```

### 9.3 é£é™©åˆ†æ

#### é«˜é£é™©é¡¹
1. **3Dæ„è±¡ç”Ÿæˆå¤±è´¥** (æ¦‚ç‡: 20%)
   - ç¼“è§£ç­–ç•¥: ä½¿ç”¨2D+ç‰©åŒ–æ€§è´¨æ›¿ä»£
   - æ€§èƒ½å½±å“: -5-7%

2. **å¯¹æ¯”å­¦ä¹ ä¸æ”¶æ•›** (æ¦‚ç‡: 15%)
   - ç¼“è§£ç­–ç•¥: è°ƒæ•´æ¸©åº¦å‚æ•°å’Œé˜Ÿåˆ—å¤§å°
   - æ€§èƒ½å½±å“: -8-10%

3. **ç®—åŠ›ä¸è¶³** (æ¦‚ç‡: 10%)
   - ç¼“è§£ç­–ç•¥: å‡å°æ¨¡å‹è§„æ¨¡æˆ–ä½¿ç”¨æ··åˆç²¾åº¦
   - æ—¶é—´å½±å“: +30-50%

#### ä¸­é£é™©é¡¹
1. **è¿‡æ‹Ÿåˆ** (æ¦‚ç‡: 30%)
   - ç¼“è§£ç­–ç•¥: æ›´å¼ºçš„æ­£åˆ™åŒ–ã€Early Stopping
   - æ€§èƒ½å½±å“: -3-5%

2. **è¶…å‚æ•°è°ƒä¼˜å›°éš¾** (æ¦‚ç‡: 25%)
   - ç¼“è§£ç­–ç•¥: ä½¿ç”¨æˆç†Ÿçš„è¶…å‚æ•°é…ç½®
   - æ—¶é—´å½±å“: +20%

---

## 10. æ€»ç»“ä¸å»ºè®®

### 10.1 æ ¸å¿ƒç­–ç•¥

ğŸ¯ **ä¸‰å¤§æ ¸å¿ƒæ”¹è¿›**:
1. **æ¶æ„å‡çº§**: Graphormer + Performer â†’ +10-12%
2. **å¯¹æ¯”å­¦ä¹ **: SimCLR/MoCo for Graphs â†’ +8-10%
3. **3D+ç‰©åŒ–ç‰¹å¾**: å¤šæ¨¡æ€èåˆ â†’ +8-12%

**é¢„æœŸæ€»æå‡**: **35-47%**

### 10.2 ä¼˜å…ˆçº§å»ºè®®

#### ğŸ”¥ é«˜ä¼˜å…ˆçº§ï¼ˆå¿…åšï¼‰
- Graphormeræ¶æ„
- å¯¹æ¯”å­¦ä¹ 
- æ·±åº¦å’Œéšè—ç»´åº¦æå‡
- Adapter/LoRAå¾®è°ƒ

#### â­ ä¸­ä¼˜å…ˆçº§ï¼ˆæ¨èï¼‰
- 3Dæ„è±¡ç‰¹å¾
- è¯¾ç¨‹å­¦ä¹ 
- å¤šä»»åŠ¡å­¦ä¹ 
- é«˜çº§æ­£åˆ™åŒ–

#### ğŸ’¡ ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰
- PerformeråŠ é€Ÿ
- Few-shotå­¦ä¹ 
- åºåˆ—ç¼–ç 
- é›†æˆå­¦ä¹ 

### 10.3 æˆåŠŸå…³é”®

1. **å……åˆ†çš„æ¶ˆèå®éªŒ**: éªŒè¯æ¯ä¸ªæ”¹è¿›çš„æœ‰æ•ˆæ€§
2. **æ¸è¿›å¼å¼€å‘**: å…ˆæ˜“åéš¾ï¼Œé€æ­¥è¿­ä»£
3. **å¯†åˆ‡ç›‘æ§**: é¿å…è¿‡æ‹Ÿåˆï¼ŒåŠæ—¶è°ƒæ•´
4. **æ–‡æ¡£å®Œå–„**: è®°å½•æ‰€æœ‰å®éªŒå’Œè¶…å‚æ•°

### 10.4 é¢„æœŸæˆæœ

å¦‚æœæŒ‰è®¡åˆ’å®æ–½ï¼Œåœ¨ **6å‘¨**å†…ï¼š
- âœ… åœ¨æ‰€æœ‰ä»»åŠ¡ä¸Šè¶…è¶ŠPepLand **35-47%**
- âœ… æ€»ç®—åŠ›æ¶ˆè€—: **300å°æ—¶** (4Ã—4090)
- âœ… æ€»æˆæœ¬: Â¥660 (ç”µè´¹)
- âœ… äº§å‡º: é¡¶ä¼šçº§è®ºæ–‡ + å¼€æºæ¨¡å‹

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-10-14
**é¢„è®¡å®æ–½æ—¶é—´**: 6å‘¨
**ç®—åŠ›éœ€æ±‚**: 4Ã—RTX 4090
**é¢„æœŸæå‡**: **+35-47%**
