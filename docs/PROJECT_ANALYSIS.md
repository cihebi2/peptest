# PepLand é¡¹ç›®æ·±åº¦åˆ†ææ–‡æ¡£

## ç›®å½•
- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [é¡¹ç›®æ¶æ„](#é¡¹ç›®æ¶æ„)
- [æ•°æ®æ”¶é›†ä¸å¤„ç†](#æ•°æ®æ”¶é›†ä¸å¤„ç†)
- [æ¨¡å‹æ¶æ„è®¾è®¡](#æ¨¡å‹æ¶æ„è®¾è®¡)
- [è®­ç»ƒç­–ç•¥](#è®­ç»ƒç­–ç•¥)
- [æ¨¡å‹è¯„ä¼°](#æ¨¡å‹è¯„ä¼°)
- [ä»£ç å®Œæ•´æ€§åˆ†æ](#ä»£ç å®Œæ•´æ€§åˆ†æ)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)

---

## é¡¹ç›®æ¦‚è¿°

### ç ”ç©¶èƒŒæ™¯
PepLand æ˜¯ä¸€ä¸ªé’ˆå¯¹è‚½æ®µï¼ˆpeptideï¼‰è¡¨ç¤ºå­¦ä¹ çš„å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸“é—¨è®¾è®¡ç”¨äºå¤„ç†åŒ…å«è§„èŒƒæ°¨åŸºé…¸ï¼ˆcanonical amino acidsï¼‰å’Œéè§„èŒƒæ°¨åŸºé…¸ï¼ˆnon-canonical amino acidsï¼‰çš„è‚½æ®µåˆ†å­ã€‚

### æ ¸å¿ƒåˆ›æ–°
1. **å¤šè§†å›¾å¼‚æ„å›¾ç¥ç»ç½‘ç»œï¼ˆMulti-view Heterogeneous Graph Neural Networkï¼‰**
   - åŒæ—¶å»ºæ¨¡åŸå­çº§ï¼ˆatom-levelï¼‰å’Œç‰‡æ®µçº§ï¼ˆfragment-levelï¼‰çš„åˆ†å­è¡¨ç¤º
   - èåˆå¤šä¸ªç²’åº¦çš„ç»“æ„ä¿¡æ¯

2. **è‡ªé€‚åº”åˆ†ç‰‡ç®—æ³•ï¼ˆAdaFragï¼‰**
   - Amiiboç®—å­ï¼šä¿ç•™é…°èƒºé”®çš„åŒæ—¶åˆ‡å‰²è‚½æ®µ
   - BRICSç®—æ³•ï¼šè¿›ä¸€æ­¥ç»†åŒ–å¤§å‹ä¾§é“¾ç»“æ„

3. **ä¸¤é˜¶æ®µé¢„è®­ç»ƒç­–ç•¥**
   - ç¬¬ä¸€é˜¶æ®µï¼šåœ¨è§„èŒƒæ°¨åŸºé…¸æ•°æ®ä¸Šé¢„è®­ç»ƒ
   - ç¬¬äºŒé˜¶æ®µï¼šåœ¨éè§„èŒƒæ°¨åŸºé…¸æ•°æ®ä¸Šç»§ç»­è®­ç»ƒ

### è®ºæ–‡ä¿¡æ¯
- æ ‡é¢˜ï¼šPepLand: a large-scale pre-trained peptide representation model for a comprehensive landscape of both canonical and non-canonical amino acids
- arXiv: https://arxiv.org/abs/2311.04419

---

## é¡¹ç›®æ¶æ„

### ç›®å½•ç»“æ„
```
pepland/
â”œâ”€â”€ configs/              # é…ç½®æ–‡ä»¶ç›®å½•
â”‚   â”œâ”€â”€ pretrain_masking.yaml  # é¢„è®­ç»ƒé…ç½®
â”‚   â”œâ”€â”€ inference.yaml         # æ¨ç†é…ç½®
â”‚   â””â”€â”€ *.json                 # å…¶ä»–ä»»åŠ¡é…ç½®
â”œâ”€â”€ model/                # æ¨¡å‹æ ¸å¿ƒä»£ç 
â”‚   â”œâ”€â”€ model.py         # PharmHGTæ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ hgt.py           # HGTå±‚å®ç°
â”‚   â”œâ”€â”€ data.py          # æ•°æ®å¤„ç†å’ŒåŠ è½½
â”‚   â”œâ”€â”€ core.py          # ç‰¹å¾æå–å™¨å’Œé¢„æµ‹å™¨
â”‚   â””â”€â”€ util.py          # å·¥å…·å‡½æ•°
â”œâ”€â”€ tokenizer/           # åˆ†å­åˆ†ç‰‡å·¥å…·
â”‚   â”œâ”€â”€ pep2fragments.py # åˆ†ç‰‡ç®—æ³•å®ç°
â”‚   â””â”€â”€ vocabs/          # ç‰‡æ®µè¯æ±‡è¡¨
â”œâ”€â”€ utils/               # é€šç”¨å·¥å…·
â”‚   â”œâ”€â”€ metrics.py       # è¯„ä¼°æŒ‡æ ‡
â”‚   â”œâ”€â”€ distribution.py  # åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·
â”‚   â”œâ”€â”€ std_logger.py    # æ—¥å¿—è®°å½•
â”‚   â””â”€â”€ utils.py         # é€šç”¨å‡½æ•°
â”œâ”€â”€ data/                # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ pretrained/      # é¢„è®­ç»ƒæ•°æ®ï¼ˆè§„èŒƒæ°¨åŸºé…¸ï¼‰
â”‚   â”œâ”€â”€ further_training/# è¿›ä¸€æ­¥è®­ç»ƒæ•°æ®ï¼ˆéè§„èŒƒæ°¨åŸºé…¸ï¼‰
â”‚   â””â”€â”€ eval/            # è¯„ä¼°æ•°æ®é›†
â”œâ”€â”€ cpkt/                # æ¨¡å‹æ£€æŸ¥ç‚¹
â”œâ”€â”€ test/                # æµ‹è¯•ä»£ç 
â”œâ”€â”€ trainer.py           # è®­ç»ƒå™¨å®ç°
â”œâ”€â”€ pretrain_masking.py  # é¢„è®­ç»ƒä¸»è„šæœ¬
â”œâ”€â”€ inference.py         # æ¨ç†è„šæœ¬
â”œâ”€â”€ splitters.py         # æ•°æ®é›†åˆ’åˆ†å·¥å…·
â””â”€â”€ environment.yaml     # ç¯å¢ƒä¾èµ–
```

### æ ¸å¿ƒæ¨¡å—å…³ç³»
```
pretrain_masking.py (ä¸»å…¥å£)
    â†“
trainer.py (è®­ç»ƒå¾ªç¯)
    â†“
model/model.py (PharmHGTæ¨¡å‹)
    â†“
model/hgt.py (HGTå±‚) + model/data.py (æ•°æ®åŠ è½½)
    â†“
tokenizer/pep2fragments.py (åˆ†å­åˆ†ç‰‡)
```

---

## æ•°æ®æ”¶é›†ä¸å¤„ç†

### 1. æ•°æ®æ¥æº

#### é¢„è®­ç»ƒæ•°æ®
- **ä½ç½®**: `data/pretrained/`
- **å†…å®¹**: åŒ…å«è§„èŒƒæ°¨åŸºé…¸çš„è‚½æ®µSMILESæ•°æ®
- **æ ¼å¼**: CSVæ–‡ä»¶ï¼ˆtrain.csv, valid.csv, test.csvï¼‰
- **å­—æ®µ**: `smiles` åˆ—

#### è¿›ä¸€æ­¥è®­ç»ƒæ•°æ®
- **ä½ç½®**: `data/further_training/`
- **å†…å®¹**: åŒ…å«éè§„èŒƒæ°¨åŸºé…¸çš„è‚½æ®µSMILESæ•°æ®
- **æ ¼å¼**: CSVæ–‡ä»¶ï¼ˆtrain.csv, valid.csv, test.csvï¼‰

#### è¯„ä¼°æ•°æ®é›†
- **ä½ç½®**: `data/eval/`
- **æ•°æ®é›†**:
  - `c-binding.csv`: è§„èŒƒæ°¨åŸºé…¸ç»“åˆæ•°æ®
  - `nc-binding.csv`: éè§„èŒƒæ°¨åŸºé…¸ç»“åˆæ•°æ®
  - `c-CPP.txt`: è§„èŒƒæ°¨åŸºé…¸ç»†èƒç©¿é€è‚½ï¼ˆCell-Penetrating Peptidesï¼‰
  - `nc-CPP.csv`: éè§„èŒƒæ°¨åŸºé…¸CPP
  - `c-Sol.txt`: è§„èŒƒæ°¨åŸºé…¸æº¶è§£åº¦æ•°æ®

### 2. æ•°æ®å¤„ç†æµç¨‹

#### åˆ†å­åˆ†ç‰‡ï¼ˆTokenizationï¼‰
**æ ¸å¿ƒç®—æ³•ä½ç½®**: `tokenizer/pep2fragments.py`

**AdaFragç®—æ³•æµç¨‹**:
```python
1. è¯†åˆ«é…°èƒºé”®ç»“æ„: C(=O)N
2. Amiiboåˆ‡å‰²:
   - ä¿ç•™é…°èƒºé”®
   - åˆ‡å‰²C-Cå•é”®å’ŒC-Nå•é”®ï¼ˆéé…°èƒºé”®ï¼‰
3. BRICSç»†åŒ–ï¼ˆå¯é€‰ï¼‰:
   - å¯¹å¤§å‹ä¾§é“¾åº”ç”¨BRICSç®—æ³•è¿›ä¸€æ­¥åˆ‡å‰²
```

**å…³é”®å‡½æ•°**:
- `get_cut_bond_idx(mol, side_chain_cut=True)`: è·å–éœ€è¦åˆ‡å‰²çš„é”®ç´¢å¼•
  - `side_chain_cut=True`: ä½¿ç”¨AdaFragï¼ˆAmiibo + BRICSï¼‰
  - `side_chain_cut=False`: ä»…ä½¿ç”¨Amiibo
- `cut_peptide(mol, patt)`: åˆ‡å‰²è‚½æ®µå¹¶è¿”å›æœ‰åºç‰‡æ®µ
- `brics_molecule(mol)`: åº”ç”¨BRICSç®—æ³•åˆ‡å‰²åˆ†å­

#### å›¾æ„å»ºï¼ˆGraph Constructionï¼‰
**ä»£ç ä½ç½®**: `model/data.py:496-632`

**æ„å»ºæµç¨‹**:
```python
Mol2HeteroGraph(mol, frag='258'):
    1. åˆ†å­åˆ†ç‰‡ â†’ è·å–ç‰‡æ®µï¼ˆfragmentsï¼‰
    2. æ„å»ºå¼‚æ„å›¾èŠ‚ç‚¹:
       - 'a' (atom): åŸå­èŠ‚ç‚¹
       - 'p' (pharm/fragment): ç‰‡æ®µèŠ‚ç‚¹
       - 'junc' (junction): è¿æ¥èŠ‚ç‚¹ï¼ˆéšå¼ï¼‰
    3. æ„å»ºå¼‚æ„å›¾è¾¹:
       - ('a', 'b', 'a'): åŸå­é—´çš„é”®
       - ('p', 'r', 'p'): ç‰‡æ®µé—´çš„ååº”è¾¹
       - ('a', 'j', 'p'): åŸå­åˆ°ç‰‡æ®µçš„è¿æ¥
       - ('p', 'j', 'a'): ç‰‡æ®µåˆ°åŸå­çš„è¿æ¥
    4. æå–ç‰¹å¾:
       - åŸå­ç‰¹å¾: 42ç»´ (åŸå­ç±»å‹ã€åº¦ã€ç”µè·ã€æ‰‹æ€§ã€æ‚åŒ–ç­‰)
       - é”®ç‰¹å¾: 14ç»´ (é”®ç±»å‹ã€å…±è½­æ€§ã€ç¯çŠ¶ç­‰)
       - ç‰‡æ®µç‰¹å¾: 196ç»´ (MACCSæŒ‡çº¹ + è¯æ•ˆå›¢æ€§è´¨)
    5. è¿”å›DGLå¼‚æ„å›¾
```

**ç‰¹å¾è¯¦æƒ…**:
```python
# åŸå­ç‰¹å¾ (42ç»´)
atom_features = [
    one_hot(atomic_num, 9ç§å…ƒç´ ),      # 10ç»´
    one_hot(degree, [0-5]),             # 6ç»´
    one_hot(formal_charge, [-2,-1,0,1,2]), # 5ç»´
    one_hot(chiral_tag, [0-3]),         # 4ç»´
    one_hot(num_Hs, [0-4]),             # 5ç»´
    one_hot(hybridization, 5ç§ç±»å‹),    # 6ç»´
    [is_aromatic],                      # 1ç»´
    [mass * 0.01]                       # 1ç»´
]

# ç‰‡æ®µç‰¹å¾ (196ç»´)
fragment_features = [
    maccs_keys,                         # 167ç»´ (MACCSåˆ†å­æŒ‡çº¹)
    [padding],                          # 1ç»´
    pharmacophore_properties,           # 27ç»´ (è¯æ•ˆå›¢æ€§è´¨)
    [padding]                           # 1ç»´
]

# é”®ç‰¹å¾ (14ç»´)
bond_features = [
    [not_none],                         # 1ç»´
    [is_single, is_double, is_triple, is_aromatic], # 4ç»´
    [is_conjugated],                    # 1ç»´
    [in_ring],                          # 1ç»´
    one_hot(stereo, [0-5])              # 7ç»´
]
```

#### æ•°æ®å¢å¼ºï¼ˆMaskingï¼‰
**ä»£ç ä½ç½®**: `model/data.py:330-493` (`MaskAtom` ç±»)

**ä¸‰ç§Maskingç­–ç•¥**:
```python
1. Random Atom Masking (mask_rate=0.8):
   - éšæœºé®è”½80%çš„åŸå­ç‰¹å¾
   - ç”¨ç‰¹æ®Šçš„maskç‰¹å¾å‘é‡æ›¿æ¢

2. Amino Acid-based Masking (mask_amino=0.3):
   - æŒ‰æ°¨åŸºé…¸å•å…ƒé®è”½
   - éšæœºé€‰æ‹©30%çš„æ°¨åŸºé…¸ï¼Œé®è”½å…¶æ‰€æœ‰åŸå­

3. Peptide Side Chain Masking (mask_pep=0.8):
   - ä¸“é—¨é®è”½ä¾§é“¾åŸå­
   - ä¿ç•™ä¸»é“¾ç»“æ„ï¼Œé®è”½80%çš„ä¾§é“¾åŸå­

4. Fragment Masking (mask_pharm=True):
   - é®è”½ç‰‡æ®µèŠ‚ç‚¹ç‰¹å¾
   - ä½¿ç”¨maskç‰‡æ®µç‰¹å¾æ›¿æ¢

5. Edge Masking (mask_edge=False):
   - å¯é€‰ï¼šé®è”½è¿æ¥åˆ°è¢«maskåŸå­çš„è¾¹
   - é»˜è®¤å…³é—­
```

#### æ•°æ®åŠ è½½å™¨
**ä»£ç ä½ç½®**: `model/data.py:635-789`

**ä¸»è¦ç»„ä»¶**:
```python
# 1. å¯è¿­ä»£æ•°æ®é›†
class MolGraphSet(IterableDataset):
    - æ”¯æŒå¤šè¿›ç¨‹æ•°æ®åŠ è½½
    - å®æ—¶å°†SMILESè½¬æ¢ä¸ºDGLå›¾
    - åº”ç”¨æ•°æ®å¢å¼ºå˜æ¢

# 2. DataLoaderåˆ›å»º
make_loaders(cfg, ddp, dataset, ...):
    - åˆ›å»ºtrain/valid/teståŠ è½½å™¨
    - æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼ˆDDPï¼‰
    - ä½¿ç”¨DGLçš„GraphDataLoader

# é…ç½®å‚æ•°
batch_size: 512         # æ‰¹æ¬¡å¤§å°
num_workers: 8          # æ•°æ®åŠ è½½è¿›ç¨‹æ•°
```

### 3. æ•°æ®é›†åˆ’åˆ†

**ä»£ç ä½ç½®**: `splitters.py`

**æ”¯æŒçš„åˆ’åˆ†æ–¹å¼**:
1. **Random Split**: éšæœºåˆ’åˆ†ï¼ˆé»˜è®¤ï¼‰
   - `random_split(dataset, frac_train=0.9, frac_valid=0.05, frac_test=0.05)`

2. **Scaffold Split**: åŸºäºåˆ†å­éª¨æ¶çš„åˆ’åˆ†
   - `scaffold_split(dataset, smiles_list, ...)`
   - ä½¿ç”¨Bemis-Murckoéª¨æ¶
   - ç¡®ä¿ç»“æ„ç›¸ä¼¼çš„åˆ†å­åœ¨åŒä¸€é›†åˆ

3. **Random Scaffold Split**: éšæœºéª¨æ¶åˆ’åˆ†
   - `random_scaffold_split(dataset, smiles_list, ...)`

---

## æ¨¡å‹æ¶æ„è®¾è®¡

### 1. æ•´ä½“æ¶æ„ï¼šPharmHGT

**ä»£ç ä½ç½®**: `model/model.py:320-415`

```python
PharmHGT(
    hid_dim=300,        # éšè—å±‚ç»´åº¦
    act='ReLU',         # æ¿€æ´»å‡½æ•°
    depth=5,            # æ¶ˆæ¯ä¼ é€’å±‚æ•°
    atom_dim=42,        # åŸå­ç‰¹å¾ç»´åº¦
    bond_dim=14,        # é”®ç‰¹å¾ç»´åº¦
    pharm_dim=196,      # ç‰‡æ®µç‰¹å¾ç»´åº¦
    reac_dim=14         # ååº”è¾¹ç‰¹å¾ç»´åº¦
)
```

**æ¶æ„ç»„ä»¶**:
```
è¾“å…¥: DGLå¼‚æ„å›¾
    â†“
ç‰¹å¾åˆå§‹åŒ–å±‚
    â†“
å¤šè§†å›¾æ¶ˆæ¯ä¼ é€’ (MVMP) Ã— 5å±‚
    â†“
å›¾è¯»å‡ºå±‚ (Node_GRU)
    â†“
è¾“å‡º: åŸå­è¡¨ç¤º + ç‰‡æ®µè¡¨ç¤º
```

### 2. æ ¸å¿ƒæ¨¡å—è¯¦è§£

#### 2.1 å¤šè§†å›¾æ¶ˆæ¯ä¼ é€’ï¼ˆMVMPï¼‰
**ä»£ç ä½ç½®**: `model/model.py:174-318`

**è®¾è®¡æ€æƒ³**:
- åŒæ—¶åœ¨åŸå­è§†å›¾å’Œç‰‡æ®µè§†å›¾ä¸Šè¿›è¡Œæ¶ˆæ¯ä¼ é€’
- é€šè¿‡junctionèŠ‚ç‚¹è¿æ¥ä¸¤ä¸ªè§†å›¾
- å®ç°è·¨è§†å›¾ä¿¡æ¯äº¤äº’

**å®ç°ç»†èŠ‚**:
```python
class MVMP(nn.Module):
    def __init__(self, hid_dim=300, depth=3, view='apj'):
        # view='apj': atom + pharm + junction

        # 1. åŒæ„è¾¹ç±»å‹ï¼ˆhomogeneous edgesï¼‰
        homo_etypes = [
            ('a', 'b', 'a'),  # atom-bond-atom
            ('p', 'r', 'p')   # pharm-reaction-pharm
        ]

        # 2. å¼‚æ„è¾¹ç±»å‹ï¼ˆheterogeneous edgesï¼‰
        hetero_etypes = [
            ('a', 'j', 'p'),  # atom-junction-pharm
            ('p', 'j', 'a')   # pharm-junction-atom
        ]

        # 3. æ³¨æ„åŠ›æœºåˆ¶
        self.attn = MultiHeadedAttention(n_heads=4, d_model=hid_dim)

        # 4. æ¶ˆæ¯ä¼ é€’å±‚
        self.mp_list = nn.ModuleList([
            nn.Linear(hid_dim, hid_dim)
            for _ in range(depth-1)
        ])

    def forward(self, bg):
        # è¿­ä»£æ¶ˆæ¯ä¼ é€’
        for i in range(depth-1):
            # (1) åŒæ„è¾¹æ¶ˆæ¯ä¼ é€’
            bg.multi_update_all(homo_update_funcs)

            # (2) å¼‚æ„è¾¹æ¶ˆæ¯ä¼ é€’
            apply_custom_copy_src(bg, hetero_etypes)

            # (3) è¾¹ç‰¹å¾æ›´æ–°
            update_edge_features(bg)

        # æœ€ç»ˆèŠ‚ç‚¹ç‰¹å¾æ›´æ–°
        final_node_update(bg)
```

**æ¶ˆæ¯ä¼ é€’æœºåˆ¶**:
```
æ—¶åˆ» t:
    èŠ‚ç‚¹ç‰¹å¾: h_v^t
    è¾¹ç‰¹å¾: e_uv^t

æ¶ˆæ¯è®¡ç®—:
    m_uv = Attention(h_u^t, h_v^t) Ã— e_uv^t

æ¶ˆæ¯èšåˆ:
    m_v = Î£_{uâˆˆN(v)} m_uv

èŠ‚ç‚¹æ›´æ–°:
    h_v^{t+1} = MLP([h_v^t || m_v || f_v])
    å…¶ä¸­ f_v æ˜¯åˆå§‹ç‰¹å¾
```

#### 2.2 å¼‚æ„å›¾Transformerï¼ˆHGTï¼‰
**ä»£ç ä½ç½®**: `model/hgt.py:12-182`

**HGTå±‚è®¾è®¡**:
```python
class HGTLayer(nn.Module):
    def __init__(self, in_dim, out_dim, node_dict, edge_dict, n_heads=4):
        # 1. ç±»å‹ç‰¹å®šçš„çº¿æ€§å˜æ¢
        self.k_linears = nn.ModuleList()  # æ¯ç§èŠ‚ç‚¹ç±»å‹çš„KçŸ©é˜µ
        self.q_linears = nn.ModuleList()  # æ¯ç§èŠ‚ç‚¹ç±»å‹çš„QçŸ©é˜µ
        self.v_linears = nn.ModuleList()  # æ¯ç§èŠ‚ç‚¹ç±»å‹çš„VçŸ©é˜µ

        # 2. å…³ç³»ç‰¹å®šçš„æ³¨æ„åŠ›çŸ©é˜µ
        self.relation_att = nn.Parameter(
            torch.Tensor(num_relations, n_heads, d_k, d_k)
        )
        self.relation_msg = nn.Parameter(
            torch.Tensor(num_relations, n_heads, d_k, d_k)
        )

        # 3. å…³ç³»ä¼˜å…ˆçº§
        self.relation_pri = nn.Parameter(
            torch.ones(num_relations, n_heads)
        )
```

**HGTæ³¨æ„åŠ›è®¡ç®—**:
```
å¯¹äºè¾¹ (u, e, v):

1. ç±»å‹ç‰¹å®šæŠ•å½±:
   K_u = W_k^{type(u)} h_u
   Q_v = W_q^{type(v)} h_v
   V_u = W_v^{type(u)} h_u

2. å…³ç³»ç‰¹å®šå˜æ¢:
   K'_u = K_u Ã— R_att^e
   V'_u = V_u Ã— R_msg^e

3. æ³¨æ„åŠ›å¾—åˆ†:
   Î±_{uv} = softmax((Q_v Â· K'_u) / âˆšd_k Ã— R_pri^e)

4. æ¶ˆæ¯èšåˆ:
   m_v = Î£_{uâˆˆN(v)} Î±_{uv} Ã— V'_u

5. èŠ‚ç‚¹æ›´æ–°:
   h_v^{new} = LayerNorm(Î± Ã— MLP(m_v) + (1-Î±) Ã— h_v^{old})
```

#### 2.3 å›¾è¯»å‡ºå±‚ï¼ˆGraph Readoutï¼‰
**ä»£ç ä½ç½®**: `model/model.py:89-158`

**Node_GRUè®¾è®¡**:
```python
class Node_GRU(nn.Module):
    def __init__(self, hid_dim, bidirectional=True):
        # 1. å¤šå¤´æ³¨æ„åŠ›æ··åˆ
        self.att_mix = MultiHeadedAttention(6, hid_dim)

        # 2. åŒå‘GRU
        self.gru = nn.GRU(
            hid_dim, hid_dim,
            batch_first=True,
            bidirectional=bidirectional
        )

    def forward(self, bg, suffix='h'):
        # (1) è·å–åŸå­å’Œç‰‡æ®µè¡¨ç¤º
        p_pharmj = split_batch(bg, 'p', f'f_{suffix}')
        a_pharmj = split_batch(bg, 'a', f'f_{suffix}')

        # (2) è·¨è§†å›¾æ³¨æ„åŠ›
        h = self.att_mix(a_pharmj, p_pharmj, p_pharmj, mask)
        h = h + a_pharmj  # æ®‹å·®è¿æ¥

        # (3) åŒå‘GRUå¤„ç†åºåˆ—
        h, hidden = self.gru(h)

        # (4) èšåˆä¸ºå›¾çº§è¡¨ç¤º
        graph_embed = mean_pooling(h)

        return graph_embed
```

### 3. ç‰¹å¾æå–å™¨å’Œé¢„æµ‹å™¨

#### 3.1 PepLandFeatureExtractor
**ä»£ç ä½ç½®**: `model/core.py:89-243`

**åŠŸèƒ½**:
```python
class PepLandFeatureExtractor(nn.Module):
    def __init__(self, model_path, pooling='avg'):
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.model = load_model(model_path)

        # ç§»é™¤ä»»åŠ¡ç‰¹å®šå±‚
        remove_layers(['readout', 'out'])

        # æ± åŒ–ç­–ç•¥
        if pooling == 'avg':
            pooling_layer = AdaptiveAvgPool1d
        elif pooling == 'max':
            pooling_layer = AdaptiveMaxPool1d
        elif pooling == 'gru':
            pooling_layer = Node_GRU

    def forward(self, input_smiles):
        # (1) SMILES â†’ å›¾
        graphs = self.tokenize(input_smiles)

        # (2) æå–åŸå­å’Œç‰‡æ®µè¡¨ç¤º
        atom_rep, frag_rep = self.model(batch_graphs)

        # (3) æ± åŒ–ä¸ºè‚½æ®µçº§è¡¨ç¤º
        pep_embeds = self.pooling_layer(
            concat([atom_rep, frag_rep])
        )

        return pep_embeds  # [batch_size, 300]
```

**ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–**:
```python
# LRUç¼“å­˜æœºåˆ¶
self._tokenize_cache = OrderedDict()
self.max_cache_size = 100000

# é¿å…é‡å¤è®¡ç®—SMILES â†’ å›¾çš„è½¬æ¢
```

#### 3.2 PropertyPredictor
**ä»£ç ä½ç½®**: `model/core.py:245-298`

**ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ**:
```python
class PropertyPredictor(nn.Module):
    def __init__(self, model_path, hidden_dims=[256, 128]):
        # ç‰¹å¾æå–å™¨ï¼ˆå†»ç»“æƒé‡ï¼‰
        self.feature_model = PepLandFeatureExtractor(
            model_path, pooling='avg'
        )

        # MLPé¢„æµ‹å¤´
        self.mlp = nn.Sequential(
            nn.Linear(300, 256), nn.ReLU(), nn.Dropout(0.1),
            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.1),
            nn.Linear(128, 1),
            nn.Sigmoid()
        )

    def forward(self, input_smiles):
        graph_rep = self.feature_model(input_smiles)
        prediction = self.mlp(graph_rep)
        return prediction
```

### 4. æ¨¡å‹å‚æ•°ç»Ÿè®¡

```python
# PharmHGTæ¨¡å‹å‚æ•°
Total Parameters: ~5-10M (å–å†³äºå…·ä½“é…ç½®)

# å„æ¨¡å—å‚æ•°åˆ†å¸ƒ
- ç‰¹å¾åˆå§‹åŒ–å±‚: ~0.2M
  - w_atom: 42 Ã— 300 = 12.6K
  - w_bond: 14 Ã— 300 = 4.2K
  - w_pharm: 196 Ã— 300 = 58.8K
  - w_junc: 238 Ã— 300 = 71.4K

- MVMPå±‚ (Ã—5): ~4M
  - æ³¨æ„åŠ›å±‚: 4 Ã— (300 Ã— 300 Ã— 4) = 1.44M per layer
  - MLPå±‚: 2 Ã— (300 Ã— 300) = 0.18M per layer

- è¯»å‡ºå±‚: ~0.5M
  - GRU: 2 Ã— (300 Ã— 300 Ã— 6) = 1.08M

- é¢„æµ‹å¤´: ~0.2M
  - Linear: 1200 Ã— 300 + 300 Ã— num_tasks
```

---

## è®­ç»ƒç­–ç•¥

### 1. ä¸¤é˜¶æ®µè®­ç»ƒç­–ç•¥

#### é˜¶æ®µ1: è§„èŒƒæ°¨åŸºé…¸é¢„è®­ç»ƒ
**é…ç½®**: `configs/pretrain_masking.yaml`
```yaml
train:
  dataset: pretrained        # ä½¿ç”¨è§„èŒƒæ°¨åŸºé…¸æ•°æ®
  model: PharmHGT           # ä»å¤´è®­ç»ƒ
  batch_size: 512
  epochs: 50
  lr: 0.001
  num_layer: 5
  hid_dim: 300
  mask_rate: 0.8
  mask_pharm: True
  mask_pep: 0.8
```

**è®­ç»ƒå‘½ä»¤**:
```bash
python pretrain_masking.py
```

#### é˜¶æ®µ2: éè§„èŒƒæ°¨åŸºé…¸ç»§ç»­è®­ç»ƒ
**é…ç½®ä¿®æ”¹**:
```yaml
train:
  dataset: further_training   # ä½¿ç”¨éè§„èŒƒæ°¨åŸºé…¸æ•°æ®
  model: fine-tune           # åŠ è½½é˜¶æ®µ1æ¨¡å‹
inference:
  model_path: ./inference/cpkt/  # é˜¶æ®µ1æ¨¡å‹è·¯å¾„
```

**è®­ç»ƒå‘½ä»¤**:
```bash
python pretrain_masking.py  # ä½¿ç”¨æ›´æ–°åçš„é…ç½®
```

### 2. è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡

**ä»£ç ä½ç½®**: `trainer.py:51-160` (`train_epoch` æ–¹æ³•)

#### ä¸»è¦é¢„è®­ç»ƒä»»åŠ¡:
```python
# 1. åŸå­æ©ç é¢„æµ‹ï¼ˆMasked Atom Predictionï¼‰
loss_atom = CrossEntropy(
    pred_atom,           # æ¨¡å‹é¢„æµ‹çš„åŸå­ç±»å‹
    true_atom_label      # çœŸå®åŸå­ç±»å‹ (119ç±»)
)

# 2. ç‰‡æ®µæ©ç é¢„æµ‹ï¼ˆMasked Fragment Predictionï¼‰
if cfg.train.mask_pharm:
    loss_pharm = CrossEntropy(
        pred_pharm,      # æ¨¡å‹é¢„æµ‹çš„ç‰‡æ®µç±»å‹
        true_pharm_label # çœŸå®ç‰‡æ®µç±»å‹ (264ç±»ç‰‡æ®µè¯æ±‡)
    )

# 3. è¾¹æ©ç é¢„æµ‹ï¼ˆMasked Edge Predictionï¼‰
if cfg.train.mask_edge:
    loss_edge = CrossEntropy(
        pred_edge,       # æ¨¡å‹é¢„æµ‹çš„è¾¹ç±»å‹
        true_edge_label  # çœŸå®è¾¹ç±»å‹ (4ç±»)
    )

# æ€»æŸå¤±
total_loss = loss_atom + loss_pharm + loss_edge
```

**é¢„æµ‹å¤´è®¾è®¡**:
```python
# åŸå­é¢„æµ‹å¤´
linear_pred_atoms = nn.Linear(300, 119)  # 119ç§åŸå­ç±»å‹

# ç‰‡æ®µé¢„æµ‹å¤´
linear_pred_pharms = nn.Linear(300, 264)  # 264ç§ç‰‡æ®µ

# é”®é¢„æµ‹å¤´
linear_pred_bonds = nn.Linear(300, 4)  # 4ç§é”®ç±»å‹
```

### 3. ä¼˜åŒ–ç­–ç•¥

**ä¼˜åŒ–å™¨é…ç½®**:
```python
# Adamä¼˜åŒ–å™¨
optimizer = optim.Adam(
    model.parameters(),
    lr=0.001,           # å­¦ä¹ ç‡
    weight_decay=0      # L2æ­£åˆ™åŒ–ï¼ˆé»˜è®¤å…³é—­ï¼‰
)

# æ¯ä¸ªæ¨¡å—ç‹¬ç«‹ä¼˜åŒ–å™¨
optimizer_model = optim.Adam(model.parameters(), lr=0.001)
optimizer_pred_atoms = optim.Adam(linear_pred_atoms.parameters(), lr=0.001)
optimizer_pred_pharms = optim.Adam(linear_pred_pharms.parameters(), lr=0.001)
optimizer_pred_bonds = optim.Adam(linear_pred_bonds.parameters(), lr=0.001)
```

**å­¦ä¹ ç‡ç­–ç•¥**:
```python
# æ³¨é‡Šä»£ç ä¸­æåˆ°æ”¯æŒPlateauè°ƒåº¦å™¨ï¼ˆå½“å‰æœªæ¿€æ´»ï¼‰
# scheduler = ReduceLROnPlateau(
#     optimizer,
#     mode='max',
#     factor=0.5,
#     patience=5
# )
```

### 4. è®­ç»ƒç›‘æ§

#### æ—¥å¿—è®°å½•
**ä»£ç ä½ç½®**: `trainer.py:134-154`
```python
# MLflowæ—¥å¿—
if cfg.logger.log:
    mlflow.log_metric("train/loss", loss, step=global_step)
    mlflow.log_metric("train/acc_atom", acc_atom, step=global_step)
    mlflow.log_metric("train/acc_pharm", acc_pharm, step=global_step)
    mlflow.log_metric("train/acc_edge", acc_edge, step=global_step)

# æ§åˆ¶å°æ—¥å¿—
Logger.info(f"train | epoch: {epoch} step: {step} | loss: {loss:.4f}")
```

#### éªŒè¯è¯„ä¼°
**ä»£ç ä½ç½®**: `trainer.py:161-271`
```python
# æ¯500æ­¥è¯„ä¼°ä¸€æ¬¡
if (global_train_step + 1) % 500 == 0:
    eval_epoch("valid")
    eval_epoch("test")

# æ¯ä¸ªepochç»“æŸè¯„ä¼°
eval_epoch("valid")
eval_epoch("test")
```

#### æ¨¡å‹ä¿å­˜
**ä»£ç ä½ç½®**: `trainer.py:210-270`
```python
# ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯é›†acc_atomï¼‰
if metrics['valid_acc_atom'] >= best_metric:
    best_metric = metrics['valid_acc_atom']

    # ä¿å­˜4ä¸ªæ¨¡å‹ç»„ä»¶
    mlflow.pytorch.save_model(model, path='model')
    mlflow.pytorch.save_model(linear_pred_atoms, path='linear_pred_atoms')
    mlflow.pytorch.save_model(linear_pred_pharms, path='linear_pred_pharms')
    mlflow.pytorch.save_model(linear_pred_bonds, path='linear_pred_bonds')
```

### 5. åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ

**DDPé…ç½®**:
```python
# å¯ç”¨DDP
if cfg.mode.ddp:
    # åˆå§‹åŒ–è¿›ç¨‹ç»„
    setup_multinodes(local_rank, world_size)

    # åŒ…è£…æ¨¡å‹
    model = DDP(
        model,
        device_ids=[global_rank],
        output_device=global_rank
    )

    # åˆ†å¸ƒå¼é‡‡æ ·å™¨
    train_sampler = DistributedSampler(
        train_dataset,
        num_replicas=world_size,
        rank=global_rank
    )

    # åŒæ­¥æŒ‡æ ‡
    torch.distributed.all_reduce(loss_accum, op=ReduceOp.SUM)
    loss_accum /= world_size
```

**ç¯å¢ƒå˜é‡**:
```bash
export LOCAL_RANK=0
export RANK=0
export WORLD_SIZE=1
```

### 6. è¶…å‚æ•°æ€»ç»“

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| batch_size | 512 | æ‰¹æ¬¡å¤§å° |
| epochs | 50 | è®­ç»ƒè½®æ•° |
| lr | 0.001 | å­¦ä¹ ç‡ |
| decay | 0 | æƒé‡è¡°å‡ |
| num_layer | 5 | GNNå±‚æ•° |
| hid_dim | 300 | éšè—å±‚ç»´åº¦ |
| atom_dim | 42 | åŸå­ç‰¹å¾ç»´åº¦ |
| bond_dim | 14 | é”®ç‰¹å¾ç»´åº¦ |
| pharm_dim | 196 | ç‰‡æ®µç‰¹å¾ç»´åº¦ |
| mask_rate | 0.8 | æ©ç æ¯”ä¾‹ |
| mask_pharm | True | æ˜¯å¦æ©ç ç‰‡æ®µ |
| mask_pep | 0.8 | ä¾§é“¾æ©ç æ¯”ä¾‹ |
| mask_edge | False | æ˜¯å¦æ©ç è¾¹ |
| num_workers | 8 | æ•°æ®åŠ è½½è¿›ç¨‹æ•° |

---

## æ¨¡å‹è¯„ä¼°

### 1. é¢„è®­ç»ƒè¯„ä¼°æŒ‡æ ‡

**ä»£ç ä½ç½®**: `trainer.py:272-367` (`evaluate` æ–¹æ³•)

#### æ ¸å¿ƒæŒ‡æ ‡:
```python
metrics = {
    # æŸå¤±
    'loss': avg_loss,

    # åŸå­é¢„æµ‹å‡†ç¡®ç‡
    'acc_atom': correct_atoms / total_atoms,

    # ç‰‡æ®µé¢„æµ‹å‡†ç¡®ç‡
    'acc_pharm': correct_pharms / total_pharms,

    # è¾¹é¢„æµ‹å‡†ç¡®ç‡
    'acc_edge': correct_edges / total_edges
}
```

**å‡†ç¡®ç‡è®¡ç®—**:
```python
def compute_accuracy(pred, target):
    pred_class = torch.argmax(pred, dim=-1)
    correct = (pred_class == target).sum().item()
    total = target.size(0)
    accuracy = correct / total
    return accuracy
```

### 2. ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°

**ä»£ç ä½ç½®**: `utils/metrics.py`

#### å›å½’ä»»åŠ¡æŒ‡æ ‡ï¼ˆäº²å’ŒåŠ›é¢„æµ‹ï¼‰:
```python
class AffinityMetrics:
    def __call__(self, pred, true):
        return {
            # å‡æ–¹è¯¯å·®
            'mse': mean_squared_error(pred, true),

            # çš®å°”é€Šç›¸å…³ç³»æ•°
            'pearson': pearsonr(pred, true)[0],

            # æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°
            'spearman': spearmanr(pred, true)[0],

            # Top-Kå¬å›ç‡
            'recall@K': recall_at_k(pred, true, k)
        }
```

#### åˆ†ç±»ä»»åŠ¡æŒ‡æ ‡:
```python
class MulticlassMetrics:
    def __call__(self, pred, true):
        return {
            # AUC-ROC
            'auc_score': roc_auc_score(true, pred, average='macro'),

            # ç²¾ç¡®ç‡
            'precision': precision_score(true, pred, average='macro'),

            # å¬å›ç‡
            'recall': recall_score(true, pred, average='macro'),

            # åˆ†ç±»æŠ¥å‘Š
            'classification_report': classification_report(true, pred)
        }
```

### 3. è¯„ä¼°æ•°æ®é›†

æ ¹æ®`data/eval/`ç›®å½•å†…å®¹ï¼š

| æ•°æ®é›† | ä»»åŠ¡ç±»å‹ | æè¿° | æ ·æœ¬æ•° |
|--------|----------|------|--------|
| c-binding.csv | å›å½’ | è§„èŒƒæ°¨åŸºé…¸è›‹ç™½è´¨ç»“åˆ | ~4M |
| nc-binding.csv | å›å½’ | éè§„èŒƒæ°¨åŸºé…¸è›‹ç™½è´¨ç»“åˆ | ~550K |
| c-CPP.txt | åˆ†ç±» | è§„èŒƒæ°¨åŸºé…¸ç»†èƒç©¿é€æ€§ | ~50K |
| nc-CPP.csv | åˆ†ç±» | éè§„èŒƒæ°¨åŸºé…¸ç»†èƒç©¿é€æ€§ | ~16M |
| c-Sol.txt | å›å½’ | è§„èŒƒæ°¨åŸºé…¸æº¶è§£åº¦ | - |

### 4. è¯„ä¼°æµç¨‹

#### é¢„è®­ç»ƒè¯„ä¼°:
```python
# è®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°
for epoch in range(epochs):
    train_epoch()

    # æ¯500æ­¥è¯„ä¼°ä¸€æ¬¡
    if step % 500 == 0:
        valid_metrics = evaluate("valid")
        test_metrics = evaluate("test")

    # æ¯è½®ç»“æŸè¯„ä¼°
    valid_metrics = evaluate("valid")
    test_metrics = evaluate("test")

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if valid_metrics['acc_atom'] > best_metric:
        save_model()
```

#### ä¸‹æ¸¸ä»»åŠ¡è¯„ä¼°:
```python
# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
feature_extractor = PepLandFeatureExtractor(model_path)

# æ„å»ºé¢„æµ‹å™¨
predictor = PropertyPredictor(model_path)

# å¾®è°ƒå’Œè¯„ä¼°
for epoch in range(finetune_epochs):
    train_step()
    metrics = evaluate(test_loader)
```

### 5. å¯è§†åŒ–

æ ¹æ®`data/eval/`ä¸­çš„PNGæ–‡ä»¶ï¼š

1. **t-SNEç‰¹å¾åµŒå…¥å¯è§†åŒ–**
   - æ–‡ä»¶: `t-SNE_Feature_Embeddings_*.png`
   - å±•ç¤ºä¸åŒæ•°æ®é›†çš„ç‰¹å¾åˆ†å¸ƒ
   - ä½¿ç”¨çœŸå®æ ‡ç­¾å’Œèšç±»ç»“æœç€è‰²

2. **ç†åŒ–æ€§è´¨åˆ†æ**
   - æ–‡ä»¶: `*_physicochemical_properties.png`
   - åˆ†æè‚½æ®µçš„ç†åŒ–æ€§è´¨åˆ†å¸ƒ
   - åŒ…æ‹¬CPPå’Œbindingæ•°æ®

3. **æ¡ˆä¾‹ç ”ç©¶**
   - æ–‡ä»¶: `case_study_predicted_scores_*.png`
   - é¢„æµ‹åˆ†æ•°ä¸çœŸå®å€¼å¯¹æ¯”

---

## ä»£ç å®Œæ•´æ€§åˆ†æ

### 1. æ ¸å¿ƒåŠŸèƒ½å®Œæ•´æ€§

#### âœ… å®Œæ•´å®ç°çš„æ¨¡å—:

1. **æ•°æ®å¤„ç†æ¨¡å—** (`model/data.py`)
   - âœ… åˆ†å­åˆ†ç‰‡ç®—æ³•ï¼ˆAdaFragï¼‰
   - âœ… å¼‚æ„å›¾æ„å»º
   - âœ… æ•°æ®å¢å¼ºï¼ˆå¤šç§maskingç­–ç•¥ï¼‰
   - âœ… æ•°æ®åŠ è½½å™¨ï¼ˆæ”¯æŒå¤šè¿›ç¨‹å’ŒDDPï¼‰

2. **æ¨¡å‹æ¶æ„** (`model/model.py`, `model/hgt.py`)
   - âœ… PharmHGTå®Œæ•´å®ç°
   - âœ… å¤šè§†å›¾æ¶ˆæ¯ä¼ é€’ï¼ˆMVMPï¼‰
   - âœ… HGTå±‚å®ç°
   - âœ… å›¾è¯»å‡ºå±‚ï¼ˆNode_GRUï¼‰

3. **è®­ç»ƒæ¡†æ¶** (`trainer.py`, `pretrain_masking.py`)
   - âœ… ä¸¤é˜¶æ®µé¢„è®­ç»ƒæµç¨‹
   - âœ… è‡ªç›‘ç£å­¦ä¹ ä»»åŠ¡
   - âœ… åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼ˆDDPï¼‰
   - âœ… æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

4. **æ¨ç†æ¥å£** (`inference.py`, `model/core.py`)
   - âœ… ç‰¹å¾æå–å™¨ï¼ˆPepLandFeatureExtractorï¼‰
   - âœ… ä¸‹æ¸¸ä»»åŠ¡é¢„æµ‹å™¨ï¼ˆPropertyPredictorï¼‰
   - âœ… æ‰¹é‡æ¨ç†æ”¯æŒ

5. **è¯„ä¼°å·¥å…·** (`utils/metrics.py`)
   - âœ… å›å½’ä»»åŠ¡æŒ‡æ ‡
   - âœ… åˆ†ç±»ä»»åŠ¡æŒ‡æ ‡
   - âœ… Top-Kå¬å›ç‡

6. **åˆ†ç‰‡å·¥å…·** (`tokenizer/pep2fragments.py`)
   - âœ… Amiiboç®—å­
   - âœ… BRICSç®—æ³•é›†æˆ
   - âœ… ç‰‡æ®µæ’åºå’Œæ ‡å‡†åŒ–

### 2. ä¾èµ–å®Œæ•´æ€§

**ç¯å¢ƒæ–‡ä»¶**: `environment.yaml`

**æ ¸å¿ƒä¾èµ–**:
```yaml
# æ·±åº¦å­¦ä¹ æ¡†æ¶
- pytorch=2.2.0 (CUDA 11.8)
- dgl=2.0.0 (CUDA 11.8)

# åŒ–å­¦ä¿¡æ¯å­¦
- rdkit=2023.3.1

# ç§‘å­¦è®¡ç®—
- numpy=1.22.4
- scipy=1.7.1
- pandas=1.4.3
- scikit-learn=1.1.1

# å®éªŒç®¡ç†
- mlflow=1.28.0

# é…ç½®ç®¡ç†
- hydra-core=1.2.0
- omegaconf=2.2.3

# åˆ†å¸ƒå¼è®­ç»ƒ
- nccl=2.12.12.1
```

**å®‰è£…å‘½ä»¤**:
```bash
conda env create -f environment.yaml
conda activate peppi  # æˆ– multiview
```

### 3. é…ç½®æ–‡ä»¶å®Œæ•´æ€§

**ä¸»é…ç½®**: `configs/pretrain_masking.yaml`
- âœ… æ¨¡å‹è¶…å‚æ•°
- âœ… è®­ç»ƒé…ç½®
- âœ… æ•°æ®é…ç½®
- âœ… æ—¥å¿—é…ç½®

**æ¨ç†é…ç½®**: `configs/inference.yaml`
- âœ… æ¨¡å‹è·¯å¾„
- âœ… æ¨ç†å‚æ•°
- âœ… è¾“å…¥è¾“å‡ºé…ç½®

### 4. ç¼ºå¤±æˆ–ä¸å®Œæ•´çš„éƒ¨åˆ†

#### âš ï¸ æ–‡æ¡£ç¼ºå¤±:
- âŒ è¯¦ç»†çš„APIæ–‡æ¡£
- âŒ ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒæ•™ç¨‹
- âŒ æ•°æ®æ ¼å¼è¯´æ˜
- âš ï¸ ç¤ºä¾‹ä»£ç è¾ƒå°‘

#### âš ï¸ æµ‹è¯•ä¸è¶³:
- âš ï¸ `test/` ç›®å½•å­˜åœ¨ä½†å†…å®¹æœªçŸ¥
- âŒ å•å…ƒæµ‹è¯•è¦†ç›–ä¸è¶³
- âŒ é›†æˆæµ‹è¯•ç¼ºå¤±

#### âš ï¸ æ•°æ®ç¼ºå¤±:
- âš ï¸ é¢„è®­ç»ƒæ•°æ®ï¼ˆ`data/pretrained/`ï¼‰éœ€è¦ç”¨æˆ·æä¾›
- âš ï¸ è¿›ä¸€æ­¥è®­ç»ƒæ•°æ®ï¼ˆ`data/further_training/`ï¼‰éœ€è¦ç”¨æˆ·æä¾›
- âœ… è¯„ä¼°æ•°æ®é›†ï¼ˆ`data/eval/`ï¼‰å·²æä¾›

#### âš ï¸ é¢„è®­ç»ƒæ¨¡å‹:
- âš ï¸ `cpkt/` ç›®å½•æœ‰æ¨¡å‹ä½†è·¯å¾„éœ€è¦é…ç½®
- âš ï¸ READMEä¸­æåˆ°çš„é¢„è®­ç»ƒæƒé‡ä¸‹è½½é“¾æ¥å¯èƒ½éœ€è¦æ›´æ–°

#### âš ï¸ å¯ç”¨æ€§é—®é¢˜:
```python
# trainer.pyä¸­çš„æ³¨é‡Šä»£ç 
# å­¦ä¹ ç‡è°ƒåº¦å™¨æœªæ¿€æ´»
# scheduler = ...

# æ°¨åŸºé…¸æ©ç é¢„æµ‹æœªæ¿€æ´»
# linear_pred_amino = ...
```

### 5. ä»£ç è´¨é‡

#### âœ… ä¼˜ç‚¹:
- ä»£ç ç»“æ„æ¸…æ™°ï¼Œæ¨¡å—åŒ–è‰¯å¥½
- ä½¿ç”¨Hydraè¿›è¡Œé…ç½®ç®¡ç†
- æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
- æœ‰MLflowæ—¥å¿—æ”¯æŒ

#### âš ï¸ æ”¹è¿›ç©ºé—´:
- æ³¨é‡Šä¸»è¦ç”¨è‹±æ–‡ï¼Œä½†ä¸å¤Ÿè¯¦ç»†
- éƒ¨åˆ†åŠŸèƒ½è¢«æ³¨é‡Šæ‰ï¼ˆå¦‚amino acidé¢„æµ‹ï¼‰
- é”™è¯¯å¤„ç†å¯ä»¥æ›´å®Œå–„
- éœ€è¦æ›´å¤šç±»å‹æç¤º

### 6. å¯è¿è¡Œæ€§è¯„ä¼°

#### é¢„è®­ç»ƒ:
```bash
# å‰ææ¡ä»¶:
1. âœ… ç¯å¢ƒå®‰è£…å®Œæˆ
2. âš ï¸ å‡†å¤‡é¢„è®­ç»ƒæ•°æ®ï¼ˆSMILES CSVæ–‡ä»¶ï¼‰
3. âœ… é…ç½®æ–‡ä»¶æ­£ç¡®
4. âš ï¸ è®¾ç½®MLflowç¯å¢ƒå˜é‡ï¼ˆå¦‚æœå¯ç”¨æ—¥å¿—ï¼‰

# è¿è¡Œå‘½ä»¤:
python pretrain_masking.py

# é¢„æœŸè¾“å‡º:
- è®­ç»ƒæ—¥å¿—
- æ¨¡å‹æ£€æŸ¥ç‚¹
- MLflowè®°å½•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
```

#### æ¨ç†:
```bash
# å‰ææ¡ä»¶:
1. âœ… ç¯å¢ƒå®‰è£…å®Œæˆ
2. âš ï¸ é¢„è®­ç»ƒæ¨¡å‹æƒé‡
3. âœ… è¾“å…¥SMILESæ–‡ä»¶

# è¿è¡Œå‘½ä»¤:
python inference.py

# æˆ–ä½¿ç”¨Python API:
from model.core import PepLandFeatureExtractor

model = PepLandFeatureExtractor(model_path, pooling='avg')
embeddings = model(['CCO', 'CCN'])
```

---

## ä½¿ç”¨æŒ‡å—

### 1. ç¯å¢ƒé…ç½®

```bash
# 1. å…‹éš†ä»“åº“
git clone <repository_url>
cd pepland

# 2. åˆ›å»ºcondaç¯å¢ƒ
conda env create -f environment.yaml

# 3. æ¿€æ´»ç¯å¢ƒ
conda activate peppi

# 4. éªŒè¯å®‰è£…
python -c "import torch; import dgl; import rdkit; print('OK')"
```

### 2. æ•°æ®å‡†å¤‡

#### é¢„è®­ç»ƒæ•°æ®æ ¼å¼:
```csv
smiles
CC(C)C[C@H](NC(=O)...)C(=O)O
CC[C@H](C)[C@H](NC(=O)...)C(=O)O
...
```

#### ç›®å½•ç»“æ„:
```
data/
â”œâ”€â”€ pretrained/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ valid.csv
â”‚   â””â”€â”€ test.csv
â””â”€â”€ further_training/
    â”œâ”€â”€ train.csv
    â”œâ”€â”€ valid.csv
    â””â”€â”€ test.csv
```

### 3. è®­ç»ƒæ¨¡å‹

#### é˜¶æ®µ1: è§„èŒƒæ°¨åŸºé…¸é¢„è®­ç»ƒ
```bash
# ä¿®æ”¹é…ç½®
vim configs/pretrain_masking.yaml

# å…³é”®é…ç½®:
train:
  dataset: pretrained
  model: PharmHGT
  batch_size: 512
  epochs: 50

# å¯åŠ¨è®­ç»ƒ
python pretrain_masking.py

# DDPè®­ç»ƒï¼ˆå¤šGPUï¼‰
torchrun --nproc_per_node=4 pretrain_masking.py
```

#### é˜¶æ®µ2: éè§„èŒƒæ°¨åŸºé…¸ç»§ç»­è®­ç»ƒ
```bash
# ä¿®æ”¹é…ç½®
vim configs/pretrain_masking.yaml

# å…³é”®é…ç½®:
train:
  dataset: further_training
  model: fine-tune
inference:
  model_path: ./outputs/.../model_step_xxx/

# å¯åŠ¨è®­ç»ƒ
python pretrain_masking.py
```

### 4. æ¨¡å‹æ¨ç†

#### æ–¹å¼1: ä½¿ç”¨è„šæœ¬
```bash
# å‡†å¤‡è¾“å…¥æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªSMILESï¼‰
echo -e "CCO\nCCN\nCCC" > input_smiles.txt

# é…ç½®æ¨ç†å‚æ•°
vim configs/inference.yaml

inference:
  model_path: ./cpkt/model/
  data: input_smiles.txt
  pool: avg  # avg, max, gru, or null
  device_ids: [0]

# è¿è¡Œæ¨ç†
python inference.py
```

#### æ–¹å¼2: ä½¿ç”¨Python API
```python
from model.core import PepLandFeatureExtractor

# åˆå§‹åŒ–æ¨¡å‹
model = PepLandFeatureExtractor(
    model_path='./cpkt/model/',
    pooling='avg'  # 'avg', 'max', 'gru', or None
)
model.eval()

# æå–ç‰¹å¾
smiles_list = ['CCO', 'CCN', 'CCC']
embeddings = model(smiles_list)
print(embeddings.shape)  # [3, 300]

# æå–åŸå­å’Œç‰‡æ®µè¡¨ç¤º
atom_embeds, frag_embeds = model.extract_atom_fragment_embedding(smiles_list)
print(atom_embeds.shape)  # [3, num_atoms, 300]
print(frag_embeds.shape)  # [3, num_frags, 300]

# æå–ç‰¹å®šåŸå­çš„è¡¨ç¤º
atom_embed = model(smiles_list, atom_index=0)  # ç¬¬0ä¸ªåŸå­
print(atom_embed.shape)  # [3, 1, 300]
```

### 5. ä¸‹æ¸¸ä»»åŠ¡å¾®è°ƒ

```python
from model.core import PropertyPredictor
import torch.nn as nn
import torch.optim as optim

# 1. åˆå§‹åŒ–é¢„æµ‹å™¨
predictor = PropertyPredictor(
    model_path='./cpkt/model/',
    pooling='avg',
    hidden_dims=[256, 128],
    mlp_dropout=0.1
)

# 2. å‡†å¤‡æ•°æ®
train_smiles = [...]
train_labels = [...]

# 3. è®­ç»ƒ
optimizer = optim.Adam(predictor.parameters(), lr=0.001)
criterion = nn.BCELoss()

for epoch in range(epochs):
    for batch_smiles, batch_labels in dataloader:
        pred = predictor(batch_smiles)
        loss = criterion(pred, batch_labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

# 4. é¢„æµ‹
with torch.no_grad():
    predictions = predictor(test_smiles)
```

### 6. åˆ†å­åˆ†ç‰‡å·¥å…·

```python
from tokenizer.pep2fragments import get_cut_bond_idx
from rdkit import Chem

# åŠ è½½åˆ†å­
smiles = 'CC(C)C[C@H](NC(=O)...)C(=O)O'
mol = Chem.MolFromSmiles(smiles)

# AdaFragåˆ†ç‰‡ï¼ˆAmiibo + BRICSï¼‰
break_bonds, break_bonds_atoms = get_cut_bond_idx(
    mol,
    side_chain_cut=True
)

# ä»…Amiiboåˆ†ç‰‡
break_bonds, break_bonds_atoms = get_cut_bond_idx(
    mol,
    side_chain_cut=False
)

# å¯è§†åŒ–åˆ‡å‰²é”®
from rdkit.Chem import Draw
highlight_bonds = break_bonds
img = Draw.MolToImage(
    mol,
    highlightBonds=highlight_bonds,
    size=(1000, 1000)
)
img.show()
```

### 7. å¸¸è§é—®é¢˜

#### Q1: å†…å­˜ä¸è¶³
```python
# è§£å†³æ–¹æ¡ˆ:
# 1. å‡å°batch_size
train.batch_size: 256  # ä»512å‡å°

# 2. å‡å°‘num_workers
train.num_workers: 4  # ä»8å‡å°

# 3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
```

#### Q2: CUDAé”™è¯¯
```bash
# æ£€æŸ¥CUDAç‰ˆæœ¬
nvidia-smi

# é‡æ–°å®‰è£…åŒ¹é…çš„PyTorch
conda install pytorch=2.2.0 pytorch-cuda=11.8 -c pytorch -c nvidia
```

#### Q3: RDKité”™è¯¯
```python
# SMILESæ— æ³•è§£æ
try:
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        print(f"Invalid SMILES: {smiles}")
except Exception as e:
    print(f"Error: {e}")
```

#### Q4: MLflowé”™è¯¯
```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
export MLFLOW_TRACKING_URI=http://localhost:5000
export MLFLOW_EXPERIMENT_NAME=pepland_pretrain

# æˆ–åœ¨é…ç½®ä¸­ç¦ç”¨
logger:
  log: False
```

### 8. æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### è®­ç»ƒä¼˜åŒ–:
```python
# 1. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()

# 2. ä½¿ç”¨æ›´å¤§çš„batch size
train.batch_size: 1024  # å¦‚æœå†…å­˜å…è®¸

# 3. ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
torchrun --nproc_per_node=8 pretrain_masking.py

# 4. ä¼˜åŒ–æ•°æ®åŠ è½½
train.num_workers: 16  # å¢åŠ workeræ•°é‡
pin_memory: True       # ä½¿ç”¨å›ºå®šå†…å­˜
```

#### æ¨ç†ä¼˜åŒ–:
```python
# 1. æ‰¹é‡æ¨ç†
batch_size = 128
for i in range(0, len(smiles_list), batch_size):
    batch = smiles_list[i:i+batch_size]
    embeddings = model(batch)

# 2. ä½¿ç”¨ç¼“å­˜
model._tokenize_cache  # è‡ªåŠ¨ç¼“å­˜SMILESâ†’å›¾è½¬æ¢

# 3. åŠç²¾åº¦æ¨ç†
model.half()  # ä½¿ç”¨FP16
```

---

## æ€»ç»“

### é¡¹ç›®ä¼˜åŠ¿:
1. âœ… **åˆ›æ–°çš„æ¨¡å‹æ¶æ„**: å¤šè§†å›¾å¼‚æ„å›¾ç½‘ç»œï¼ŒåŒæ—¶å»ºæ¨¡åŸå­å’Œç‰‡æ®µ
2. âœ… **å®Œæ•´çš„è®­ç»ƒæ¡†æ¶**: ä¸¤é˜¶æ®µé¢„è®­ç»ƒï¼Œæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
3. âœ… **ä¸“é—¨çš„åˆ†ç‰‡ç®—æ³•**: AdaFragç®—æ³•é’ˆå¯¹è‚½æ®µç»“æ„ä¼˜åŒ–
4. âœ… **è‰¯å¥½çš„ä»£ç ç»“æ„**: æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•
5. âœ… **ä¸°å¯Œçš„è¯„ä¼°æ•°æ®**: æä¾›å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡æ•°æ®é›†

### éœ€è¦æ”¹è¿›:
1. âš ï¸ **æ–‡æ¡£ä¸è¶³**: ç¼ºå°‘è¯¦ç»†çš„APIæ–‡æ¡£å’Œæ•™ç¨‹
2. âš ï¸ **æµ‹è¯•è¦†ç›–**: å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•éœ€è¦è¡¥å……
3. âš ï¸ **æ•°æ®å¯è·å¾—æ€§**: é¢„è®­ç»ƒæ•°æ®éœ€è¦ç”¨æˆ·è‡ªè¡Œå‡†å¤‡
4. âš ï¸ **æ¨¡å‹æƒé‡**: é¢„è®­ç»ƒæƒé‡çš„åˆ†å‘éœ€è¦æ˜ç¡®

### å»ºè®®:
1. ğŸ“ è¡¥å……è¯¦ç»†çš„ä½¿ç”¨æ–‡æ¡£å’Œç¤ºä¾‹
2. ğŸ§ª å¢åŠ å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
3. ğŸ“¦ æä¾›é¢„è®­ç»ƒæ¨¡å‹æƒé‡çš„ä¸‹è½½é“¾æ¥
4. ğŸ“Š å¢åŠ æ›´å¤šå¯è§†åŒ–å’Œåˆ†æå·¥å…·
5. ğŸ”§ æ¿€æ´»è¢«æ³¨é‡Šçš„åŠŸèƒ½ï¼ˆå¦‚å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼‰

---

## å¼•ç”¨

å¦‚æœä½¿ç”¨PepLandï¼Œè¯·å¼•ç”¨ï¼š
```bibtex
@article{pepland2023,
  title={PepLand: a large-scale pre-trained peptide representation model for a comprehensive landscape of both canonical and non-canonical amino acids},
  author={...},
  journal={arXiv preprint arXiv:2311.04419},
  year={2023}
}
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2025-10-14
**ç»´æŠ¤è€…**: PepLand Team
