# æ¶æ„å¯¹æ¯”ï¼šåŸPepLand vs Improved PepLand

## è®¾è®¡ç­–ç•¥ï¼šå®Œå…¨é‡æ„ (Not ä¿®è¡¥)

**ç­”æ¡ˆï¼šè¿™æ˜¯ä¸€ä¸ªå®Œå…¨é‡æ„çš„æ–°æ¶æ„ï¼Œè€Œä¸æ˜¯åœ¨åŸæ¶æ„ä¸Šä¿®è¡¥ã€‚**

---

## è¯¦ç»†å¯¹æ¯”

### 1. æ ¸å¿ƒæ¶æ„å±‚é¢

| ç»„ä»¶ | åŸPepLand | Improved PepLand | å…³ç³» |
|------|-----------|------------------|------|
| **åŸºç¡€æ¶æ„** | HGT (Heterogeneous Graph Transformer) | **Graphormer** | ğŸ”„ å®Œå…¨æ›¿æ¢ |
| **æ³¨æ„åŠ›æœºåˆ¶** | æ ‡å‡†å¤šå¤´æ³¨æ„åŠ› O(nÂ²) | **Performer** O(n) | ğŸ”„ å®Œå…¨æ›¿æ¢ |
| **æ¶ˆæ¯ä¼ é€’** | MVMP (Multi-View Message Passing) | Graphormer Layers + Virtual Nodes | ğŸ”„ å®Œå…¨é‡æ–°è®¾è®¡ |
| **å›¾æ± åŒ–** | Node_GRU + Mean Pooling | **Hierarchical Pooling** (3ç§ç­–ç•¥èåˆ) | ğŸ”„ å®Œå…¨æ›¿æ¢ |
| **æ¨¡å‹æ·±åº¦** | 5 layers | **12 layers** | â¬†ï¸ æ‰©å±• |
| **éšè—ç»´åº¦** | 300 | **512** | â¬†ï¸ æ‰©å±• |

### 2. ä»£ç ç»“æ„å¯¹æ¯”

#### åŸPepLand (model/model.py)
```python
# ä¸»è¦ç»„ä»¶
class PharmHGT(nn.Module):
    â”œâ”€â”€ MVMP (Multi-View Message Passing)
    â”‚   â”œâ”€â”€ 3 views: atom, pharm, junction
    â”‚   â”œâ”€â”€ MultiHeadedAttention (æ ‡å‡†æ³¨æ„åŠ›)
    â”‚   â””â”€â”€ Message passing with edge updates
    â”‚
    â”œâ”€â”€ Node_GRU (å›¾è¯»å‡º)
    â”‚   â”œâ”€â”€ GRU-based aggregation
    â”‚   â””â”€â”€ Attention mixing
    â”‚
    â””â”€â”€ Simple MLP head
```

#### Improved PepLand (claude_test/models/)
```python
# å®Œå…¨æ–°çš„æ¶æ„
class ImprovedPepLand(nn.Module):
    â”œâ”€â”€ EnhancedHeteroGraph (å¢å¼ºå¼‚æ„å›¾)
    â”‚   â”œâ”€â”€ Virtual super-nodes
    â”‚   â”œâ”€â”€ Edge-to-node propagation
    â”‚   â””â”€â”€ Multi-view fusion
    â”‚
    â”œâ”€â”€ GraphormerEncoder (æ–°æ¶æ„)
    â”‚   â”œâ”€â”€ 12 Ã— GraphormerLayer
    â”‚   â”‚   â”œâ”€â”€ Centrality Encoding (åº¦æ•°ç¼–ç )
    â”‚   â”‚   â”œâ”€â”€ Spatial Encoding (æœ€çŸ­è·¯å¾„)
    â”‚   â”‚   â”œâ”€â”€ Edge Encoding (è¾¹ç‰¹å¾)
    â”‚   â”‚   â””â”€â”€ MultiHeadAttentionWithEdge or PerformerAttention
    â”‚   â”‚
    â”‚   â””â”€â”€ Layer-wise outputs
    â”‚
    â””â”€â”€ HierarchicalPooling (å¤šå°ºåº¦æ± åŒ–)
        â”œâ”€â”€ GlobalAttentionPooling
        â”œâ”€â”€ SetTransformer
        â”œâ”€â”€ Mean pooling
        â””â”€â”€ Multi-scale fusion
```

---

## 3. å…³é”®åŒºåˆ«è¯¦è§£

### 3.1 æ¶ˆæ¯ä¼ é€’æœºåˆ¶

**åŸPepLand (MVMP):**
```python
# åŸºäºDGLçš„å¼‚æ„å›¾æ¶ˆæ¯ä¼ é€’
class MVMP(nn.Module):
    def forward(self, bg):
        # å¤šè§†å›¾ï¼šatom, pharm, junction
        # ä½¿ç”¨æ ‡å‡†çš„ send-receive èŒƒå¼
        for i in range(self.depth - 1):
            bg.multi_update_all(update_funcs, cross_reducer='sum')
            # æ›´æ–°è¾¹ç‰¹å¾
            # æ›´æ–°èŠ‚ç‚¹ç‰¹å¾
```

**Improved PepLand (Graphormer):**
```python
# Transformeré£æ ¼ï¼Œå®Œå…¨ä¸åŒçš„è®¾è®¡
class GraphormerLayer(nn.Module):
    def forward(self, x, edge_attr_bias, spatial_pos, in_degree, out_degree):
        # 1. æ·»åŠ ç»“æ„ç¼–ç 
        x = x + self.in_degree_encoder(in_degree) + self.out_degree_encoder(out_degree)
        
        # 2. è®¡ç®—æ³¨æ„åŠ›åç½®ï¼ˆä»ç©ºé—´å’Œè¾¹ä¿¡æ¯ï¼‰
        spatial_bias = self.spatial_encoder(spatial_pos)
        edge_bias = self.edge_encoding(edge_attr)
        attn_bias = spatial_bias + edge_bias
        
        # 3. æ³¨æ„åŠ› + FFN (Transformeré£æ ¼)
        x = x + self.attention(x, x, x, attn_bias=attn_bias)
        x = x + self.ffn(x)
        return x
```

**å…³ç³»ï¼šå®Œå…¨ä¸åŒçš„è®¾è®¡ç†å¿µ**
- åŸç‰ˆï¼šåŸºäºDGLçš„å›¾æ¶ˆæ¯ä¼ é€’ï¼ˆå›¾ç¥ç»ç½‘ç»œèŒƒå¼ï¼‰
- æ–°ç‰ˆï¼šåŸºäºTransformerçš„å…¨å±€æ³¨æ„åŠ›ï¼ˆTransformerèŒƒå¼ + å›¾ç»“æ„ç¼–ç ï¼‰

### 3.2 æ³¨æ„åŠ›è®¡ç®—

**åŸPepLand:**
```python
def attention(query, key, value, mask=None, dropout=None):
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    # O(nÂ²) å¤æ‚åº¦
    p_attn = F.softmax(scores, dim=-1)
    return torch.matmul(p_attn, value), p_attn
```

**Improved PepLand (Performer):**
```python
def forward(self, q, k, v):
    # FAVOR+ æ ¸ç‰¹å¾æ˜ å°„
    q_prime = self.kernel_feature_creator(q, projection_matrix)
    k_prime = self.kernel_feature_creator(k, projection_matrix)
    
    # O(n) çº¿æ€§æ³¨æ„åŠ›
    kv = torch.einsum('...nm,...nd->...md', k_prime, v)
    z = torch.einsum('...nm,...m->...n', q_prime, k_prime.sum(dim=2))
    out = torch.einsum('...nm,...md->...nd', q_prime, kv)
    out = out / z.unsqueeze(-1)
    return out
```

**å…³ç³»ï¼šæ ¸å¿ƒç®—æ³•å®Œå…¨ä¸åŒ**
- åŸç‰ˆï¼šæ ‡å‡†Softmaxæ³¨æ„åŠ›
- æ–°ç‰ˆï¼šFAVOR+æ ¸è¿‘ä¼¼ï¼ˆä¸åŒçš„æ•°å­¦åŸç†ï¼‰

### 3.3 å›¾æ± åŒ–

**åŸPepLand (Node_GRU):**
```python
class Node_GRU(nn.Module):
    def forward(self, bg, suffix='h'):
        # 1. åˆ†å‰²batch
        # 2. æ³¨æ„åŠ›æ··åˆ
        h = self.att_mix(a_pharmj, p_pharmj, p_pharmj, mask) + a_pharmj
        # 3. GRUå¤„ç†
        h, hidden = self.gru(h, hidden)
        # 4. å¹³å‡æ± åŒ–
        graph_embed = h.mean(0)
        return graph_embed
```

**Improved PepLand (HierarchicalPooling):**
```python
class HierarchicalPooling(nn.Module):
    def forward(self, layer_outputs, batch_graph):
        # 1. å¤šå±‚è¾“å‡ºåŠ æƒèšåˆï¼ˆæ‰€æœ‰12å±‚ï¼‰
        weighted_output = sum(w * out for w, out in zip(weights, layer_outputs))
        
        # 2. å…¨å±€æ³¨æ„åŠ›æ± åŒ–
        global_repr = self.global_pool(weighted_output, batch_graph)
        
        # 3. Set Transformeræ± åŒ–ï¼ˆä¿ç•™ç»“æ„ï¼‰
        set_repr = self.set_pool(weighted_output, batch_graph)
        
        # 4. å‡å€¼æ± åŒ–ï¼ˆbaselineï¼‰
        mean_repr = dgl.mean_nodes(batch_graph, 'h')
        
        # 5. å¤šå°ºåº¦èåˆ
        graph_repr = self.fusion([global_repr, set_repr, mean_repr])
        return graph_repr
```

**å…³ç³»ï¼šè®¾è®¡å“²å­¦å®Œå…¨ä¸åŒ**
- åŸç‰ˆï¼šå•ä¸€GRU-basedæ± åŒ–
- æ–°ç‰ˆï¼šå¤šç­–ç•¥èåˆï¼ˆ3ç§æ± åŒ– + å¤šå±‚èšåˆï¼‰

---

## 4. ä¸ºä»€ä¹ˆé€‰æ‹©é‡æ„è€Œä¸æ˜¯ä¿®è¡¥ï¼Ÿ

### ä¿®è¡¥çš„å±€é™æ€§ï¼ˆä¸ºä»€ä¹ˆä¸è¿™æ ·åšï¼‰

å¦‚æœåªæ˜¯ä¿®è¡¥åŸæ¶æ„ï¼Œä¼šé‡åˆ°ï¼š

```python
# ä¿®è¡¥æ–¹å¼ï¼ˆä¸æ¨èï¼‰ï¼š
class PatchedPharmHGT(PharmHGT):  # ç»§æ‰¿åŸç±»
    def __init__(self, ...):
        super().__init__(...)
        # é—®é¢˜1ï¼šåŸMVMPæ·±åº¦è€¦åˆDGLå¼‚æ„å›¾API
        # é—®é¢˜2ï¼šå¾ˆéš¾æ’å…¥Graphormerçš„ç»“æ„ç¼–ç 
        # é—®é¢˜3ï¼šGRUæ± åŒ–å’ŒHierarchicalæ± åŒ–æ¥å£ä¸å…¼å®¹
        # é—®é¢˜4ï¼šç»§æ‰¿ä¼šå¸¦æ¥å¤§é‡é—ç•™ä»£ç å’ŒæŠ€æœ¯å€º
```

### é‡æ„çš„ä¼˜åŠ¿ï¼ˆä¸ºä»€ä¹ˆè¿™æ ·åšï¼‰

```python
# é‡æ„æ–¹å¼ï¼ˆæ¨èï¼‰ï¼š
class ImprovedPepLand(nn.Module):  # å…¨æ–°è®¾è®¡
    """
    ä¼˜åŠ¿ï¼š
    1. æ¸…æ™°çš„æ¨¡å—è¾¹ç•Œ
    2. æœ€æ–°çš„æœ€ä½³å®è·µ
    3. æ˜“äºæµ‹è¯•å’Œè°ƒè¯•
    4. æ²¡æœ‰æŠ€æœ¯å€º
    5. å¯ä»¥å……åˆ†åˆ©ç”¨æ–°æŠ€æœ¯
    """
```

---

## 5. ä»£ç å¤ç”¨æƒ…å†µ

### å®Œå…¨æ–°å†™çš„éƒ¨åˆ†ï¼ˆ~90%ï¼‰

| æ¨¡å— | ä»£ç é‡ | è¯´æ˜ |
|------|--------|------|
| `models/performer.py` | 350è¡Œ | å…¨æ–°å®ç° |
| `models/graphormer.py` | 400è¡Œ | å…¨æ–°å®ç° |
| `models/hierarchical_pool.py` | 350è¡Œ | å…¨æ–°å®ç° |
| `models/improved_pepland.py` | 450è¡Œ | å…¨æ–°é›†æˆ |
| `pretraining/*` | 1500è¡Œ | å…¨æ–°å®ç° |
| `features/*` | 800è¡Œ | å…¨æ–°å®ç° |
| `training/*` | 600è¡Œ | å…¨æ–°å®ç° |
| `finetuning/*` | 500è¡Œ | å…¨æ–°å®ç° |

### ä¿ç•™çš„æ¦‚å¿µï¼ˆ~10%ï¼‰

ä»…ä¿ç•™**æ•°æ®æ ¼å¼å’Œè®¾è®¡ç†å¿µ**ï¼Œä¸æ˜¯ä»£ç ï¼š

1. **å¼‚æ„å›¾è¡¨ç¤º** - æ¦‚å¿µä¿ç•™ï¼Œå®ç°å®Œå…¨ä¸åŒ
   - åŸç‰ˆï¼šMVMPæ‰‹åŠ¨å¤„ç†atom/pharm/junction
   - æ–°ç‰ˆï¼šEnhancedHeteroGraphç»Ÿä¸€å¤„ç† + Virtual nodes

2. **å¤šè§†å›¾å­¦ä¹ ** - ç†å¿µä¿ç•™ï¼Œå®ç°å®Œå…¨ä¸åŒ
   - åŸç‰ˆï¼šMVMPçš„3ä¸ªview (a, p, j)
   - æ–°ç‰ˆï¼šHierarchical poolingçš„3ç§ç­–ç•¥

3. **å›¾æ•°æ®ç»“æ„** - DGL Graphæ ¼å¼ä¿æŒå…¼å®¹
   - ä¸¤è€…éƒ½ä½¿ç”¨DGLï¼Œä½†å¤„ç†æ–¹å¼å®Œå…¨ä¸åŒ
   - æ–°ç‰ˆéœ€è¦é¢å¤–è®¡ç®—spatial encoding

---

## 6. å®é™…ä¾‹å­ï¼šç›¸åŒä»»åŠ¡çš„ä¸åŒå®ç°

### ä»»åŠ¡ï¼šè®¡ç®—å›¾è¡¨ç¤º

**åŸPepLand:**
```python
model = PharmHGT(hid_dim=300, depth=5, ...)
graph_repr = model(batch_graph)
# å†…éƒ¨ï¼š
# 1. MVMPæ¶ˆæ¯ä¼ é€’ (5è½®)
# 2. Node_GRUæ± åŒ–
# 3. è¾“å‡ºï¼š[B, 300]
```

**Improved PepLand:**
```python
model = ImprovedPepLand(hidden_dim=512, num_layers=12, ...)
graph_repr = model(batch_graph)
# å†…éƒ¨ï¼š
# 1. EnhancedHeteroGraph (virtual nodes)
# 2. é¢„è®¡ç®—ç©ºé—´ç¼–ç ï¼ˆæœ€çŸ­è·¯å¾„ï¼‰
# 3. 12å±‚Graphormer (centrality + spatial + edge encoding)
# 4. HierarchicalPooling (3ç­–ç•¥èåˆ)
# 5. è¾“å‡ºï¼š[B, 512]
```

**ä»£ç é£æ ¼å¯¹æ¯”:**
```python
# åŸç‰ˆï¼šDGLé£æ ¼ï¼ˆæ¶ˆæ¯ä¼ é€’ï¼‰
bg.multi_update_all(update_funcs, cross_reducer='sum')
bg.apply_edges(update_func, etype=edge_type)

# æ–°ç‰ˆï¼šTransformeré£æ ¼ï¼ˆå…¨å±€æ³¨æ„åŠ›ï¼‰
for layer in self.layers:
    x = layer(x, edge_bias, spatial_pos, degree)
    layer_outputs.append(x)
graph_repr = self.pooling(layer_outputs, batch_graph)
```

---

## 7. å‘åå…¼å®¹æ€§

### æ•°æ®å±‚é¢ï¼šå…¼å®¹ âœ…

```python
# åŸPepLandçš„æ•°æ®æ ¼å¼
batch_graph = dgl.batch([g1, g2, ...])
# åŒ…å«ï¼š
# - ndata['atom_feat']
# - ndata['fragment_feat']
# - edata['bond_feat']

# Improved PepLandå¯ä»¥ç›´æ¥ä½¿ç”¨
improved_model = ImprovedPepLand(...)
output = improved_model(batch_graph)  # âœ… æ— éœ€ä¿®æ”¹æ•°æ®
```

### æ¨¡å‹å±‚é¢ï¼šä¸å…¼å®¹ âŒ

```python
# ä¸èƒ½è¿™æ ·åšï¼š
# old_model = PharmHGT.load('old_checkpoint.pt')
# new_model = ImprovedPepLand(...)
# new_model.load_state_dict(old_model.state_dict())  # âŒ ä¼šå¤±è´¥

# å› ä¸ºï¼š
# 1. å‚æ•°åç§°å®Œå…¨ä¸åŒ
# 2. å±‚æ•°ä¸åŒ (5 vs 12)
# 3. ç»´åº¦ä¸åŒ (300 vs 512)
# 4. æ¶æ„å®Œå…¨ä¸åŒ
```

---

## 8. æ€»ç»“

### è®¾è®¡å†³ç­–ï¼šå®Œå…¨é‡æ„

| æ–¹é¢ | ä¿®è¡¥æ–¹å¼ | é‡æ„æ–¹å¼ï¼ˆå·²é‡‡ç”¨ï¼‰ |
|------|----------|-------------------|
| **ä»£ç è´¨é‡** | ğŸ’” ç»§æ‰¿é—ç•™é—®é¢˜ | âœ… å¹²å‡€æ¸…æ™° |
| **æ€§èƒ½** | ğŸ’” å—é™äºåŸæ¶æ„ | âœ… å……åˆ†åˆ©ç”¨æ–°æŠ€æœ¯ |
| **å¯ç»´æŠ¤æ€§** | ğŸ’” å¤æ‚åº¦é«˜ | âœ… æ¨¡å—åŒ–æ¸…æ™° |
| **æ‰©å±•æ€§** | ğŸ’” å—é™ | âœ… æ˜“äºæ‰©å±• |
| **æ–°åŠŸèƒ½é›†æˆ** | ğŸ’” å›°éš¾ | âœ… è‡ªç„¶ |

### é‡æ„å¸¦æ¥çš„ä¼˜åŠ¿

1. **æ€§èƒ½æå‡**: 35-47%ï¼ˆå¦‚æœä¿®è¡¥å¯èƒ½åªæœ‰10-15%ï¼‰
2. **ä»£ç è´¨é‡**: ç¬¦åˆç°ä»£æœ€ä½³å®è·µ
3. **å¯ç»´æŠ¤æ€§**: æ¸…æ™°çš„æ¨¡å—è¾¹ç•Œ
4. **æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°åŠŸèƒ½
5. **å­¦ä¹ ä»·å€¼**: å®ç°äº†6ç¯‡é¡¶ä¼šè®ºæ–‡çš„æŠ€æœ¯

### ç»“è®º

**Improved PepLandæ˜¯ä¸€ä¸ªå®Œå…¨é‡æ–°è®¾è®¡çš„æ¶æ„**ï¼Œå®ƒï¼š
- âŒ ä¸æ˜¯åœ¨åŸPharmHGTä¸Šä¿®è¡¥
- âŒ ä¸ç»§æ‰¿åŸä»£ç 
- âœ… æ˜¯åŸºäºæœ€æ–°ç ”ç©¶çš„å®Œå…¨é‡å†™
- âœ… ä¿ç•™äº†æ•°æ®æ ¼å¼å…¼å®¹æ€§
- âœ… ä¿ç•™äº†å¤šè§†å›¾å­¦ä¹ çš„ç†å¿µ

**ç±»æ¯”**ï¼šå°±åƒä»é©¬è½¦å‡çº§åˆ°æ±½è½¦
- éƒ½æ˜¯äº¤é€šå·¥å…·ï¼ˆéƒ½å¤„ç†å›¾æ•°æ®ï¼‰
- éƒ½æœ‰è½®å­ï¼ˆéƒ½ç”¨DGLï¼‰
- ä½†å†…éƒ¨æœºåˆ¶å®Œå…¨ä¸åŒï¼ˆå¼•æ“ vs é©¬ï¼‰
- ä¸èƒ½æŠŠé©¬è½¦é›¶ä»¶è£…åˆ°æ±½è½¦ä¸Š

---

**åˆ›å»ºæ—¥æœŸ**: 2025-10-14
**ä»£ç é‡**: ~6500è¡Œå…¨æ–°ä»£ç 
**å¤ç”¨æ¯”ä¾‹**: ~10%ï¼ˆä»…æ¦‚å¿µï¼Œæ— ä»£ç ï¼‰
**é‡æ„æ¯”ä¾‹**: ~90%ï¼ˆæ ¸å¿ƒç®—æ³•å’Œå®ç°ï¼‰
