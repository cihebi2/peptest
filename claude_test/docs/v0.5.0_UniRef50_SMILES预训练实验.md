# v0.5.0 UniRef50 SMILES 大规模对比学习预训练实验

**实验日期**: 2025-10-19
**实验状态**: ✅ 正在运行
**负责人**: Claude + 用户
**版本**: v0.5.0

---

## 📋 实验概述

### 实验目标

本实验旨在使用 UniRef50 数据库中的大规模肽段序列（292K条）进行对比学习预训练，学习肽段的通用表示。这是项目首次在10万级规模数据上进行预训练，相比之前的小规模实验（3K样本）有两个数量级的提升。

### 核心创新点

1. **大规模数据集**: 使用 292,052 条 UniRef50 序列，比之前规模扩大约 100 倍
2. **序列到SMILES转换**: 将氨基酸序列转换为分子SMILES表示，使模型能够学习化学结构信息
3. **懒加载机制**: 实现高效的动态数据加载，解决大规模数据的内存问题
4. **双GPU并行训练**: 同时在 GPU 0 和 GPU 1 上训练，加速实验进程

---

## 🗂️ 数据准备

### 1. 原始数据获取

**数据源**: UniRef50 (UniProt Reference Clusters)
- **路径**: `/home/qlyu/software/uniref/uniref50_5_30_20aa.fasta`
- **筛选条件**:
  - 序列长度: 5-30 个氨基酸
  - 氨基酸类型: 仅包含 20 种标准氨基酸 (ACDEFGHIKLMNPQRSTVWY)
- **序列数量**: 292,052 条
- **文件大小**: 39 MB
- **格式**: FASTA

**选择原因**:
- UniRef50 是高质量的蛋白序列聚类数据库（50%序列一致性）
- 筛选5-30 AA确保序列适合肽段研究（避免过长蛋白质序列）
- 仅使用标准氨基酸确保能成功转换为SMILES

### 2. 序列到SMILES转换

#### 转换脚本

**文件**: `convert_seq_to_smiles.py`
**核心方法**:
```python
from rdkit import Chem

def sequence_to_smiles(sequence):
    """将氨基酸序列转换为肽段SMILES"""
    # 检查序列有效性
    if not all(aa in STANDARD_AA for aa in sequence):
        return None

    # 使用RDKit将序列转换为分子对象
    mol = Chem.MolFromSequence(sequence)
    if mol is None:
        return None

    # 转换为SMILES字符串
    smiles = Chem.MolToSmiles(mol)
    return smiles
```

#### 转换过程

```bash
# 运行转换
python convert_seq_to_smiles.py
```

**输入**: 氨基酸单字母序列（如 "MKLAL"）
**输出**: 肽段SMILES（如 "CC[C@H](C)[C@H](NC(=O)..."）

#### 转换结果

| 指标 | 数值 |
|-----|------|
| 原始序列数 | 292,052 |
| 成功转换 | 292,052 (100%) |
| 转换失败 | 0 (0%) |
| 输出大小 | 130 MB |

**数据集分割**:
- **训练集** (`train.csv`): 233,641 条 (80%), 104 MB
- **验证集** (`valid.csv`): 29,205 条 (10%), 13 MB
- **测试集** (`test.csv`): 29,206 条 (10%), 13 MB

**输出路径**: `/home/qlyu/AA_peptide/pepland/data/uniref50_smiles/`

#### SMILES示例

**短肽** (5 AA: MKLAL):
```
CC[C@H](C)[C@H](NC(=O)[C@H](CCCCN)NC(=O)[C@H](CC(C)C)NC(=O)[C@@H](N)CCSC)C(=O)O
```

**长肽** (25 AA):
```
CC[C@H](C)[C@H](NC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@@H](NC(=O)CNC(=O)CNC(=O)[C@@H](NC(=O)[C@@H](NC(=O)[C@H](C)NC(=O)[C@H](CC(=O)O)NC(=O)[C@@H](NC(=O)[C@H](C)NC(=O)[C@@H](NC(=O)[C@H](CCC(=O)O)NC(=O)[C@H](CCCCN)NC(=O)CNC(=O)[C@@H](NC(=O)[C@H](CC(C)C)NC(=O)[C@H](CC(=O)O)NC(=O)[C@H](CO)NC(=O)[C@@H](NC(=O)[C@@H](N)CCSC)[C@@H](C)O)C(C)C)[C@@H](C)O)C(C)C)C(C)C)[C@@H](C)O)[C@@H](C)O)[C@@H](C)O)C(=O)N...
```

---

## 🏗️ 模型架构

### ImprovedPepLand 编码器

```yaml
模型名称: ImprovedPepLand (大型版本)
总参数量: 32,643,345 (32.6M)

输入特征:
  atom_dim: 38        # SMILES分子原子特征维度
  bond_dim: 14        # 键特征维度
  fragment_dim: 196   # 片段特征维度

架构配置:
  hidden_dim: 512     # 隐藏层维度（大型模型）
  num_layers: 8       # Transformer层数
  num_heads: 8        # 多头注意力头数
  dropout: 0.1        # Dropout率

核心技术:
  - Performer Attention: 线性复杂度注意力机制
  - Virtual Node: 虚拟节点增强图表示
  - Graphormer编码: 中心性编码 + 空间编码
  - 层次化池化: 全局注意力池化
```

### 对比学习配置

```yaml
方法: SimCLR (Simple Framework for Contrastive Learning)

投影头:
  projection_dim: 256   # 投影空间维度

对比损失:
  temperature: 0.5      # 温度参数（数值稳定）

数据增强:
  view1: 特征掩码 (mask_ratio=0.15)
  view2: 属性增强 (noise_std=0.1)
```

### 架构图

```
输入SMILES → RDKit解析 → 分子图
                           ↓
        [原子特征 + 键特征 + 片段特征]
                           ↓
              ImprovedPepLand编码器
         (Performer + Virtual Node + Graphormer)
                           ↓
                    图表示向量 (512维)
                           ↓
                     投影头 (256维)
                           ↓
                    对比学习损失
```

---

## 🔧 技术创新

### 1. 懒加载数据加载器

#### 问题分析

**原始方案** (`data_loader_real.py`):
```python
class PeptideDataset(Dataset):
    def __init__(self, csv_path):
        self.df = pd.read_csv(csv_path)

        # ❌ 预加载所有数据到内存
        self.graphs = []
        for idx, row in self.df.iterrows():
            smiles = row['smiles']
            g = smiles_to_graph(smiles)  # 耗时操作
            self.graphs.append(g)
```

**问题**:
- 初始化时间: 233K × 0.5秒/条 ≈ **32小时**
- 内存占用: 233K × 50KB/图 ≈ **11.7 GB**
- 不可扩展: 更大数据集完全无法使用

#### 解决方案

**懒加载版本** (`data_loader_lazy.py`):
```python
class PeptideDatasetLazy(Dataset):
    def __init__(self, csv_path):
        # ✅ 只读取CSV索引
        self.df = pd.read_csv(csv_path)
        # 初始化立即完成（< 1秒）

    def __getitem__(self, idx):
        # ✅ 动态转换SMILES为图
        smiles = self.df.iloc[idx]['smiles']
        graph = smiles_to_graph(smiles)
        return graph
```

**优势对比**:

| 指标 | 原始方案 | 懒加载方案 | 提升 |
|-----|---------|-----------|------|
| 初始化时间 | ~32 小时 | < 1 秒 | 100,000× |
| 内存占用 | ~11.7 GB | ~100 MB | 117× |
| 可扩展性 | 受内存限制 | 几乎无限 | ∞ |

**并行加速**:
```python
DataLoader(
    dataset,
    batch_size=16,
    num_workers=4,        # 4个进程并行转换SMILES
    persistent_workers=True
)
```

### 2. 梯度累积优化

#### 问题

- 肽段SMILES长度: 平均 500-1500 字符
- 分子图节点数: 50-200 个原子
- GPU显存限制: 24 GB

**直接使用大batch的问题**:
- `batch_size=64`: OOM（显存不足）
- `batch_size=16`: 训练不稳定（batch太小）

#### 解决方案

```python
# 配置
batch_size = 16              # 实际batch（适应显存）
accumulation_steps = 4       # 累积4次
effective_batch_size = 64    # 有效batch

# 训练循环
optimizer.zero_grad()
for batch_idx, batch in enumerate(train_loader):
    loss = model(batch)

    # 梯度累积
    loss = loss / accumulation_steps
    loss.backward()

    # 每4个batch更新一次
    if (batch_idx + 1) % accumulation_steps == 0:
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        optimizer.zero_grad()
```

**优势**:
- ✅ 等效于 `batch_size=64` 的效果
- ✅ 显存占用仅为 `batch_size=16` 水平
- ✅ 训练稳定性提升

---

## ⚙️ 训练配置

### 优化器配置

```yaml
优化器: AdamW
  learning_rate: 3e-5        # 学习率（较小，稳定训练）
  weight_decay: 1e-5         # 权重衰减（正则化）
  betas: [0.9, 0.999]        # Adam动量参数
  eps: 1e-8                  # 数值稳定性参数
```

### 学习率调度

```python
# Warmup + Cosine Annealing
def lr_lambda(epoch):
    if epoch < warmup_epochs:  # warmup_epochs = 5
        # 线性warmup
        return (epoch + 1) / warmup_epochs
    else:
        # 余弦退火
        progress = (epoch - warmup_epochs) / (num_epochs - warmup_epochs)
        return 0.5 * (1 + np.cos(np.pi * progress))
```

**学习率曲线**:
- Epoch 1-5: 0 → 3e-5 (线性warmup)
- Epoch 6-100: 3e-5 → 1.5e-6 (余弦衰减)

### 训练超参数

```yaml
批处理:
  batch_size: 16
  accumulation_steps: 4
  effective_batch_size: 64

数据加载:
  num_workers: 4
  persistent_workers: true

训练轮次:
  num_epochs: 100
  warmup_epochs: 5

梯度控制:
  gradient_clip: 1.0

模型保存:
  save_interval: 10          # 每10个epoch保存检查点
  early_stopping_patience: 15 # 15个epoch无改进则停止
```

---

## 🚀 实验执行

### 训练脚本

**文件**: `train_v5_uniref_smiles.py`

**核心流程**:
```python
def main():
    # 1. 设备配置
    device = torch.device(f'cuda:{args.gpu}')

    # 2. 数据加载（懒加载）
    dataloaders = create_dataloaders(
        data_dir='/home/qlyu/AA_peptide/pepland/data/uniref50_smiles',
        batch_size=16,
        num_workers=4
    )

    # 3. 模型创建
    encoder = ImprovedPepLand(...)
    contrastive_model = GraphContrastiveLearning(encoder, ...)

    # 4. 优化器和调度器
    optimizer = optim.AdamW(...)
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    # 5. 训练循环
    for epoch in range(num_epochs):
        loss = train_epoch(model, dataloader, optimizer, ...)
        scheduler.step()

        # 保存最佳模型
        if loss < best_loss:
            save_checkpoint(...)
```

### 启动命令

```bash
# GPU 0
CUDA_VISIBLE_DEVICES=0 nohup python -u train_v5_uniref_smiles.py --gpu 0 \
    > test_results/train_v5_uniref_gpu0.log 2>&1 &

# GPU 1
CUDA_VISIBLE_DEVICES=1 nohup python -u train_v5_uniref_smiles.py --gpu 0 \
    > test_results/train_v5_uniref_gpu1.log 2>&1 &
```

**参数说明**:
- `CUDA_VISIBLE_DEVICES`: 指定可见GPU
- `nohup`: 后台运行，断开连接后继续
- `python -u`: 禁用输出缓冲，实时写入日志
- `--gpu`: 训练脚本的GPU参数（内部使用cuda:0，因为CUDA_VISIBLE_DEVICES已映射）

### 训练启动时间

**实际启动时间**: 2025-10-19 16:45

**进程信息**:
- GPU 0: PID 59365
- GPU 1: PID 60074

---

## 📊 实验监控

### 训练进度（启动后16分钟）

| GPU | 批次进度 | 速度 | 预计Epoch时间 |
|-----|---------|------|-------------|
| 0   | 477/14603 (3.3%) | ~2.9 it/s | ~82 分钟 |
| 1   | 490/14603 (3.4%) | ~4.1 it/s | ~59 分钟 |

### GPU使用情况

| GPU | 显存使用 | 利用率 | 温度 | 功耗 |
|-----|---------|--------|------|------|
| 0   | 21.3 GB / 24 GB (89%) | 88-94% | 62-64°C | 320-325W |
| 1   | 13.4 GB / 24 GB (56%) | 78-82% | 61-69°C | 345-351W |

**观察**:
- GPU 0 显存使用较高（21.3 GB）但运行稳定
- GPU 1 显存使用较低（13.4 GB）且速度更快
- 两者利用率都很高（78-94%），训练充分利用GPU

### 日志文件

**路径**:
- `/home/qlyu/AA_peptide/pepland/claude_test/test_results/train_v5_uniref_gpu0.log`
- `/home/qlyu/AA_peptide/pepland/claude_test/test_results/train_v5_uniref_gpu1.log`

**日志内容示例**:
```
============================================================
v0.5.0 - UniRef50 SMILES 大规模预训练 (GPU 0)
============================================================

设备: cuda:0

配置:
  data_dir: /home/qlyu/AA_peptide/pepland/data/uniref50_smiles
  batch_size: 16
  ...

加载数据...
  总样本数: 233641

创建编码器...
  模型参数量: 32,643,345

开始预训练...
------------------------------------------------------------
Epoch 1:   3%|▎  | 477/14603 [02:44<1:18:21, 3.01it/s]
```

### 监控命令

```bash
# 查看训练进度
tail -f test_results/train_v5_uniref_gpu0.log

# 查看GPU状态
nvidia-smi

# 查看进程状态
ps aux | grep train_v5_uniref
```

---

## ⏱️ 时间估算

### 单Epoch时间

- **GPU 0**: ~82 分钟/epoch
- **GPU 1**: ~59 分钟/epoch
- **平均**: ~70 分钟/epoch

### 总训练时间

**计划训练**: 100 epochs

| 场景 | 时间估算 |
|-----|---------|
| 最快情况（GPU 1速度） | 59 min × 100 = 98.3 小时 ≈ **4.1 天** |
| 最慢情况（GPU 0速度） | 82 min × 100 = 136.7 小时 ≈ **5.7 天** |
| 预期（平均速度） | 70 min × 100 = 116.7 小时 ≈ **4.9 天** |

**考虑早停** (patience=15):
- 预期有效训练: 40-60 epochs
- 预期总时间: **2-3 天**

---

## 💾 模型保存

### 保存策略

```python
# 1. 最佳模型（自动保存）
if loss < best_loss:
    save_checkpoint(
        'test_results/best_v5_uniref_gpu{gpu}.pt',
        {
            'epoch': epoch,
            'encoder_state_dict': encoder.state_dict(),
            'model_state_dict': contrastive_model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict(),
            'loss': loss,
            'config': config
        }
    )

# 2. 定期检查点（每10个epoch）
if (epoch + 1) % 10 == 0:
    save_checkpoint(
        f'test_results/checkpoint_v5_uniref_gpu{gpu}_epoch_{epoch+1}.pt',
        ...
    )

# 3. 最终编码器（训练结束）
save_checkpoint(
    'test_results/final_v5_uniref_gpu{gpu}.pt',
    {
        'encoder_state_dict': encoder.state_dict(),
        'config': config,
        'losses': losses,
        'final_loss': loss
    }
)
```

### 保存文件

**GPU 0**:
- `best_v5_uniref_gpu0.pt` - 最佳模型
- `checkpoint_v5_uniref_gpu0_epoch_10.pt` - Epoch 10检查点
- `checkpoint_v5_uniref_gpu0_epoch_20.pt` - Epoch 20检查点
- ...
- `final_v5_uniref_gpu0.pt` - 最终编码器

**GPU 1**:
- `best_v5_uniref_gpu1.pt`
- `checkpoint_v5_uniref_gpu1_epoch_10.pt`
- ...
- `final_v5_uniref_gpu1.pt`

---

## 🎯 预期结果

### 训练指标

**对比学习损失**:
- 初始损失: ~4-5 (随机初始化)
- 预期最终损失: ~1.5-2.0
- 损失曲线: 单调递减，收敛稳定

**训练稳定性**:
- ✅ 无NaN损失
- ✅ 无梯度爆炸
- ✅ 学习率平滑变化
- ✅ 验证损失与训练损失趋势一致

### 学到的表示

预期编码器能学到：

1. **化学结构特征**
   - 原子类型和连接方式
   - 官能团模式
   - 分子骨架结构

2. **肽段特性**
   - 氨基酸序列模式
   - 二级结构倾向
   - 理化性质（疏水性、电荷等）

3. **语义相似性**
   - 相似序列的表示接近
   - 不同功能肽段的表示分离
   - 保留序列-结构-功能关系

---

## 🔬 下游任务评估（计划）

### 评估任务

训练完成后，将在以下任务上评估预训练编码器的质量：

#### 1. 溶解度预测
- **数据集**: 肽段溶解度数据
- **评估指标**: MAE, RMSE, R²
- **对比基线**:
  - 从头训练的模型
  - v0.2.0 小规模预训练模型

#### 2. 毒性预测
- **数据集**: 肽段毒性标注数据
- **评估指标**: AUC, F1-score, Accuracy
- **对比基线**: 同上

#### 3. 抗菌活性预测
- **数据集**: 抗菌肽（AMP）数据库
- **评估指标**: AUC, Precision, Recall
- **对比基线**: 同上

#### 4. 表示质量可视化
- **方法**: t-SNE / UMAP降维
- **评估**: 聚类质量、类别分离度
- **分析**: 学到的表示空间结构

### 评估流程

```python
# 1. 加载预训练编码器
encoder = ImprovedPepLand(...)
encoder.load_state_dict(torch.load('best_v5_uniref_gpu0.pt')['encoder_state_dict'])

# 2. 冻结编码器（可选）
for param in encoder.parameters():
    param.requires_grad = False

# 3. 添加下游任务头
model = nn.Sequential(
    encoder,
    nn.Linear(512, 256),
    nn.ReLU(),
    nn.Linear(256, 1)  # 回归任务
)

# 4. 微调
train_on_downstream_task(model, downstream_data)

# 5. 评估
evaluate(model, test_data)
```

---

## 📈 与之前版本对比

| 版本 | 数据量 | 数据格式 | 模型规模 | 训练时间 | 状态 |
|-----|-------|---------|---------|---------|------|
| v0.1.0 | 998 | SMILES | 6.8M | < 1小时 | ✅ 完成 |
| v0.2.0 | 998 | SMILES | 6.8M | < 1小时 | ✅ 完成 |
| v0.3.0 | 998 | SMILES | 6.8M | < 1小时 | ✅ 完成 |
| v0.4.0 | 2994 | SMILES | 13M | ~2小时 | ✅ 完成 |
| **v0.5.0** | **292K** | **SMILES** | **32.6M** | **~5天** | **🔄 运行中** |

**主要提升**:
- 数据量: 998 → 292,052 (**292×**)
- 模型规模: 6.8M → 32.6M (**4.8×**)
- 预期性能: 显著提升（待验证）

---

## 🐛 潜在问题与解决方案

### 1. 训练不稳定

**问题**: 损失震荡、梯度爆炸
**解决**:
- ✅ 梯度裁剪 (max_norm=1.0)
- ✅ 较小学习率 (3e-5)
- ✅ Warmup (5 epochs)
- ✅ 数值裁剪防止溢出

### 2. 过拟合

**问题**: 训练损失下降，验证损失上升
**解决**:
- ✅ Dropout (0.1)
- ✅ Weight decay (1e-5)
- ✅ 早停 (patience=15)
- ✅ 大规模数据集降低过拟合风险

### 3. 显存不足

**问题**: OOM错误
**解决**:
- ✅ 小batch size (16)
- ✅ 梯度累积 (4 steps)
- ✅ 懒加载减少内存占用

### 4. 训练速度慢

**问题**: 单epoch时间过长
**已采取措施**:
- ✅ 双GPU并行训练
- ✅ 懒加载 + 多进程 (num_workers=4)
- ✅ Performer Attention (线性复杂度)

**可能的优化** (如果需要):
- 使用更多GPU (最多6个)
- 减少num_layers (8→6)
- 使用混合精度训练 (FP16/BF16)

---

## 📝 实验日志

### 2025-10-19 16:45 - 训练启动

- ✅ 数据转换完成（100%成功率）
- ✅ 懒加载数据加载器测试通过
- ✅ 训练脚本验证通过
- ✅ GPU 0 和 GPU 1 训练启动
- ✅ 日志文件正常输出

**观察**:
- 数据加载立即完成（懒加载生效）
- 模型创建成功（32.6M参数）
- 第一个batch前向传播正常
- GPU利用率达到80%+

### 2025-10-19 17:01 - 16分钟检查点

**训练进度**:
- GPU 0: 477/14603 (3.3%), ~2.9 it/s
- GPU 1: 490/14603 (3.4%), ~4.1 it/s

**GPU状态**:
- 显存使用稳定（GPU 0: 21GB, GPU 1: 13GB）
- 温度正常（60-70°C）
- 利用率高（78-94%）

**问题**: 无
**状态**: ✅ 运行正常

---

## 🔮 后续工作

### 短期（训练完成后）

1. **结果分析**
   - 损失曲线分析
   - 学习率调度效果
   - GPU 0 vs GPU 1 性能对比

2. **下游任务评估**
   - 溶解度预测
   - 毒性预测
   - 表示可视化

3. **模型对比**
   - vs 小规模预训练 (v0.2.0)
   - vs 从头训练
   - vs 传统特征方法

### 中期

1. **模型优化**
   - 超参数调优
   - 架构改进
   - 数据增强策略

2. **扩展数据集**
   - 加入非标准氨基酸
   - 整合其他肽段数据库
   - 多任务学习

3. **应用开发**
   - 肽段性质预测工具
   - 序列设计辅助
   - 虚拟筛选平台

### 长期

1. **大模型方向**
   - 扩展到蛋白质序列（全长）
   - Transformer架构升级
   - 多模态学习（序列+结构）

2. **实验验证**
   - 与湿实验室合作
   - 候选肽段合成验证
   - 性能基准测试

---

## 📚 参考文献

### 数据来源

1. **UniRef50**
   - Suzek, B. E., et al. (2015). "UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches." Bioinformatics.

### 方法论

2. **SimCLR**
   - Chen, T., et al. (2020). "A Simple Framework for Contrastive Learning of Visual Representations." ICML.

3. **Performer**
   - Choromanski, K., et al. (2021). "Rethinking Attention with Performers." ICLR.

4. **Graphormer**
   - Ying, C., et al. (2021). "Do Transformers Really Perform Badly for Graph Representation?" NeurIPS.

### 工具

5. **RDKit**
   - RDKit: Open-source cheminformatics. http://www.rdkit.org

6. **DGL**
   - Wang, M., et al. (2019). "Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks." arXiv.

---

## 🤝 致谢

- **数据来源**: UniProt Consortium
- **开源工具**: RDKit, DGL, PyTorch
- **计算资源**: 实验室GPU集群
- **技术支持**: Claude (AI Assistant)

---

## 📄 附录

### A. 文件清单

```
claude_test/
├── train_v5_uniref_smiles.py          # 训练脚本
├── convert_seq_to_smiles.py           # 序列转SMILES
├── data_loader_lazy.py                # 懒加载数据加载器
├── models/
│   └── improved_pepland.py            # 模型架构
├── pretraining/
│   ├── contrastive.py                 # 对比学习
│   └── augmentation.py                # 数据增强
└── test_results/
    ├── train_v5_uniref_gpu0.log       # GPU 0日志
    ├── train_v5_uniref_gpu1.log       # GPU 1日志
    ├── best_v5_uniref_gpu0.pt         # GPU 0最佳模型
    ├── best_v5_uniref_gpu1.pt         # GPU 1最佳模型
    └── ...
```

### B. 环境配置

```yaml
Python: 3.11
PyTorch: 2.x
CUDA: 12.1
GPU: NVIDIA (24GB显存)

主要依赖:
  - torch
  - dgl
  - rdkit
  - pandas
  - numpy
  - tqdm
```

### C. 完整配置文件

```python
config = {
    'data_dir': '/home/qlyu/AA_peptide/pepland/data/uniref50_smiles',
    'batch_size': 16,
    'accumulation_steps': 4,
    'hidden_dim': 512,
    'num_layers': 8,
    'num_heads': 8,
    'dropout': 0.1,
    'projection_dim': 256,
    'temperature': 0.5,
    'learning_rate': 3e-5,
    'weight_decay': 1e-5,
    'num_epochs': 100,
    'max_samples': None,
    'warmup_epochs': 5,
}
```

---

**文档版本**: 1.0
**最后更新**: 2025-10-19 17:05
**状态**: 实验进行中
**下次更新**: 第一个epoch完成后
